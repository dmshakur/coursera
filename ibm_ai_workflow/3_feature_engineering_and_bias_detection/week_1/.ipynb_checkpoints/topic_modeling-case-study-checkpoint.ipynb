{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY - topic modeling and feature engineering\n",
    "\n",
    "\n",
    "[Feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) is the process of using domain knowledge of your data to create features that can be leveraged by machine learning.  That is not a hard definition, because sometimes it is used in a context where features are transformed for machine learning, but the inclusion of domain knowledge is not implied.  \n",
    "\n",
    "It is unfortunately common that for large datasets engineered features are not easy to create.  When there are many features generally only a small number play an important roll when it comes to prediction.  Furthermore,  domain insight is even more difficult to fold into the model when there are hundreds or thousands of features to keep in mind.  However, there is a middle ground---much of the worlds knowledge is locked up in language.  In this case study we will use topic modeling to gather insight from text.  Ideally, the result of these types of experiments would be shared with domain experts to further engineer features that are relevant when it comes to your business opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from string import punctuation, printable\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "try:\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.sklearn\n",
    "except:\n",
    "    raise Exception(\"'pip install pyldavis' before running this notebook\")\n",
    "\n",
    "pyLDAvis.enable_notebook()    \n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "## supress all warnings (not to be used during development)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "   >Goal:  AAVAIL has recently enabled comments on the core streaming service.  The data science team knows that   this will be an incredibly important source of data going forward.  It will be used inform customer retention, product quality, product market fit and more.  Comments are going live next week and being the diligent data scientist that you are your plan is to build a topic modeling pipeline that that will consume the comments and create visualizations that can be used to communicate with domain experts.\n",
    "  \n",
    "## Outline\n",
    "\n",
    "1. EDA - summary tables, use tSNE to visualize the data\n",
    "2. Create a transfomation pipelines for NMF and LDA\n",
    "3. Use ldaviz and wordclouds to get insight into the clusters\n",
    "\n",
    "## Data\n",
    "\n",
    "Here we see how to load the data.\n",
    "\n",
    "* [download the movie review data](http://www.nltk.org/nltk_data)\n",
    "* For more examples of applications with these data see [NLTK's book chapter that uses these data](https://www.nltk.org/book/ch06.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\"\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(\"..\",\"data\")\n",
    "movie_reviews = load_files(os.path.join(data_dir,\"movie_reviews\"), shuffle=True)\n",
    "X = movie_reviews.data\n",
    "y = movie_reviews.target\n",
    "target_names = movie_reviews.target_names\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "The main focus of this exercise is to enable visualization of topics, but these topics can be used as additional \n",
    "features for prediction tasks.  The goal of this case study is to ensure that you are comfortable with natural language processing pipelines and topic modeling tools. \n",
    "\n",
    "There are many ways to process tokens (words, dates, emojis etc).  NLTK is often used to pre-process text data before the tokens are vectorized.  Generally, the tokens are modified via [stemming or lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).  The next code block provides a lemmatization function that makes use of the library [spacy](https://spacy.io/).  You will need to install it and download the English language reference material as follows.  Stopwords are words that are very common or otherwise irrelevant we use a default list here, but it is an important part of NLP pipelines that needs to be customized for the subject area. Use the following function to process the corpus (this can take a few minutes)\n",
    "\n",
    "```bash\n",
    "~$ pip install spacy\n",
    "~$ python -m spacy download en\n",
    "```\n",
    "\n",
    "If you prefer to use NLTK then you could use a simple lemmatizer like the WordLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron fool people time people time pron fool people time abraham lincoln\n",
      "\n",
      "pron can fool some of the people all of the time and all of the people some of the time but pron can not fool all of the people all of the time abraham lincoln\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "STOPLIST = ENGLISH_STOP_WORDS\n",
    "STOPLIST = set(list(STOPLIST) + [\"foo\"])\n",
    "\n",
    "if not 'nlp' in locals():\n",
    "    print(\"Loading English Module...\")\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "def lemmatize_document(doc, stop_words=None):\n",
    "    \"\"\"\n",
    "    takes a list of strings where each string is a document\n",
    "    returns a processed list of strings\n",
    "    \"\"\"\n",
    "    \n",
    "    if not stop_words:\n",
    "        stop_words = set([])\n",
    "  \n",
    "    ## ensure working with string\n",
    "    doc = str(doc)\n",
    "\n",
    "    # First remove punctuation form string\n",
    "    if sys.version_info.major == 3:\n",
    "        PUNCT_DICT = {ord(punc): None for punc in punctuation}\n",
    "        doc = doc.translate(PUNCT_DICT)\n",
    "\n",
    "    # remove unicode\n",
    "    clean_doc = \"\".join([char for char in doc if char in printable])\n",
    "            \n",
    "    # Run the doc through spaCy\n",
    "    doc = nlp(clean_doc)\n",
    "\n",
    "    # Lemmatize and lower text\n",
    "    tokens = [re.sub(\"\\W+\",\"\",token.lemma_.lower()) for token in doc ]\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "    \n",
    "    return ' '.join(w for w in tokens if w not in stop_words)    \n",
    "\n",
    "## example usage\n",
    "corpus = ['\"You can fool some of the people all of the time, and all of the people some of the time, but you can not fool all of the people all of the time\". -- Abraham Lincoln']\n",
    "processed = [lemmatize_document(doc, STOPLIST) for doc in corpus]\n",
    "print(processed[0])\n",
    "processed = [lemmatize_document(doc, None) for doc in corpus]\n",
    "print(\"\\n\"+processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use stemming or lemmatization to process the corpus\n",
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "\n",
    "Use the CountVectorizer from sklearn to vectorize the tokens.\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "* [scikit-learn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "* [scikit-learn working with text](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "Because this is an exercise in visualization set the `max_features` to something like 1000.  In the context of supervised learning it is reasonable to grid-search to optimize this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3\n",
    "\n",
    "Use model the corpus with LDA.  For example, you could use something like the following.\n",
    "\n",
    "```python\n",
    "n_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                      learning_method='online',\n",
    "                                      learning_offset=50.,\n",
    "                                      random_state=0)\n",
    "\n",
    "lda_model.fit(tf)\n",
    "```\n",
    "\n",
    "You could use a pipeline here to make it easier to iterate on changes.\n",
    "\n",
    "* [scikit-learn's LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "* [scikit-learn's user guide for LDA](https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 4\n",
    "\n",
    "Visualize the corpus using [pyldavis](https://github.com/bmabey/pyLDAvis).\n",
    "\n",
    "```python\n",
    "pyLDAvis.sklearn.prepare(lda_model,tf, tf_vectorizer, R=20)\n",
    "```\n",
    "\n",
    "* [PyLDAViz documentation](https://pyldavis.readthedocs.io/en/latest)\n",
    "* [PyLDAViz demos](https://pyldavis.readthedocs.io/en/latest/readme.html#video-demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 5\n",
    "\n",
    "Try different numbers of clusters until there is decent separation in the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization here can help determine a reasonable number of number of clusters and it can serve as a communication tool.  If the goal was to find topics that are associated with customer profiles then you would likely work with folks in marketing to refine the clustering.  There are a couple of parameters than can be used to modify the clustering and visualization.  The discovery of meaningful topics is a form of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 6\n",
    "\n",
    "If you were to use the topics from this model to inform clustering or supervised learning you would first need to be able to extract and represent them as a matrix.  Along the same lines if you were to populate a report with tabular descriptions of the data then you will need to be able to extract topic representations.  Here is a starter function\n",
    "\n",
    "```python\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        _top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        top_words[str(topic_idx)] = _top_words\n",
    "    return(top_words)\n",
    "```\n",
    "\n",
    "Use the function to print the top k words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION (EXTRA CREDIT) 7\n",
    "\n",
    "If you used `fit_transform` on your original tokens you should have a `2000 x k` array where `k` is the number of topics you choose.  Create a PCA or tSNE visualization that projects the tf matrix into lower dimensional space then uses colors to indicate which documents belong to a topic (e.g. probability > 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://player.vimeo.com/video/87110435\" width=\"640\" height=\"360\"  frameborder=\"0\" allowfullscreen></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
