{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY - convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synopsis\n",
    "----------\n",
    "\n",
    "You were hired at AAVAIL to be a member of a data science team that works closely together.  Some of your first projects\n",
    "are meant to help marketing with customer retention and to investigate market specific trends. There are also some\n",
    "projects relating to user comments that are getting off the ground.  However, you will also be working alongside\n",
    "the deep-learning specialists that maintain the core product at AAVAIL---its audio and visual manipulation models.\n",
    "\n",
    "Because the team meets regularly all new data science hires are expected to go through a series deep-learning tutorials\n",
    "to ensure that they can contribute to conversations about the core product.   The first in this series is the following\n",
    "tutorial on CNNs.  You will be guided through the following parts.\n",
    "\n",
    "  1. Environment setup\n",
    "  2. Model scaffolding using Keras\n",
    "  3. Logging and Model serialization\n",
    "  4. Model iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST\n",
    "\n",
    ">One project that the data science team at AAVAIL has been tasked with is ensuring that the video feeds are in fact news video feeds.  There are people that are performing quality assurance on these feeds, but eventually the data science team will need to build a service that samples a number of frames from a video, then identifies objects in the images, flagging for review any feeds that may be different.\n",
    "\n",
    "A solid benchmark dataset for this task is the Fashion MNIST dataset.  \n",
    "\n",
    "* training set - 60,000 images\n",
    "* test set - 10,000 images\n",
    "* images are 28 pixels x 28 pixels\n",
    "* classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9971946936572728166\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13053817608792130541\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "## check hardware availability\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the data  \n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() \n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', \n",
    "               'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "## Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "X_train = train_images\n",
    "X_test = test_images\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "Visualize a sample of the images to QA the data set (You should scroll through several pages of images) and print a summary of the data.  Also create a base model to compare your neural network to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE (visualization code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE (Summarize the data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "\n",
    "Build a base model.  Construct a pipeline that uses PCA into a classic machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE (base model)\n",
    "\n",
    "X_train_flat = np.array([i.flatten() for i in train_images])\n",
    "X_test_flat = np.array([i.flatten() for i in test_images])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 -  model scaffolding using Keras\n",
    "\n",
    "Create a function that returns a model using ``keras.models.Sequential()`` and ensure that you pass ``activation_function`` as an argument.  Instaintiate a version of the model and print the summary.  This function is just meant to return a simple multilayer perceptron network.  At a minimum the function code should contain:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3\n",
    "\n",
    "Build a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4\n",
    "\n",
    "Create another version of your neural network.  This time you should build a proper CNN.  Remember that one pattern to consider starting from is alternating ``Con2D`` and ``MaxPooling2D`` layers.  This is often followed by a couple of ``Dense`` layers.  Recall that the the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The output of the last ``Dense`` layer should correspond to the number of classes and generally uses a 'softmax' activation.  Use `model.summary()` to ensure a cohesive architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 - logging and Model serialization\n",
    "\n",
    "You can use a trained model without having to retrain it.  Your can also continue training a model to pick-up training where you left off.  The `tf.keras.callbacks.ModelCheckpoint` callback allows to continually save the model both during and at the end of training.  For long running models this is ideal in case the training is interrupted.  Otherwise you can \n",
    "used `model.save()` and `model.load()`.  In this part you will create a function that accomplished a few things at once.  Here is some pseudocode that you could work from.\n",
    "```python\n",
    "def train_network(model_name,model,loss_fn, optimizer='adam'):\n",
    "\n",
    "    if not os.path.exists(saved model):\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=loss_fn,\n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  batch_size=64,\n",
    "                  epochs=10,\n",
    "                  validation_data=(X_test, y_test))\n",
    "        \n",
    "        create_log_file()\n",
    "        model.save(saved_model)\n",
    "    else:\n",
    "        print(\"... loading saved model\")\n",
    "        model = keras.models.load_model(saved_model)\n",
    "        \n",
    "    return(model)    \n",
    "```    \n",
    "The two things you are trying to accomplish with this function are:\n",
    "\n",
    "1. save your models so that each iteration only needs to be run once\n",
    "2. save the specifics of your model in a log file \n",
    "\n",
    "  * optimizer \n",
    "  * loss_fn \n",
    "  * test_loss\n",
    "  * test_accuracy\n",
    "  * any notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 - model iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE (build and train a MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function,sparse_categorical_crossentropy\r",
      "\r\n",
      "optimizer,adam\r",
      "\r\n",
      "test_loss,0.36062976176738737\r",
      "\r\n",
      "test_acc,0.8744\r",
      "\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE (build and train a CNN)\n",
    "\n",
    "## add the channel dimensions to your data\n",
    "X_train_1 = np.expand_dims(X_train, -1)\n",
    "X_test_1 = np.expand_dims(X_test, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function,sparse_categorical_crossentropy\r",
      "\r\n",
      "optimizer,adam\r",
      "\r\n",
      "test_loss,0.25984301267564297\r",
      "\r\n",
      "test_acc,0.9088\r",
      "\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
