{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage, misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[ 1.,  0., -1.],\n",
       "                        [ 2.,  0., -2.],\n",
       "                        [ 1.,  0., -1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.state_dict()['weight'][0][0] = torch.tensor([[1.0, 0, -1.0], [2.0, 0, -2.0], [1.0, 0.0, -1.0]])\n",
    "conv.state_dict()['bias'][0] = 0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.zeros(1, 1, 5, 5)\n",
    "image[0, 0, :, 2] = 1\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = conv(image)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 2\n",
    "conv1 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = k)\n",
    "conv1.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv1.state_dict()['bias'][0] = 0.0\n",
    "conv1.state_dict()\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "image1 = torch.ones(1, 1, m, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[4., 4., 4.],\n",
       "           [4., 4., 4.],\n",
       "           [4., 4., 4.]]]], grad_fn=<MkldnnConvolutionBackward>),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = conv1(image1)\n",
    "z1, z1.shape[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[1., 1.],\n",
       "                        [1., 1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 2, stride = 2)\n",
    "conv3.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv3.state_dict()['bias'][0] = 0.0\n",
    "conv3.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[4., 4.],\n",
       "           [4., 4.]]]], grad_fn=<MkldnnConvolutionBackward>),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3 = conv3(image1)\n",
    "z3, z3.shape[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[4.]]]], grad_fn=<MkldnnConvolutionBackward>), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv4 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 2, stride = 3)\n",
    "conv4.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv4.state_dict()['bias'][0] = 0.0\n",
    "conv4.state_dict()\n",
    "z4 = conv4(image1)\n",
    "z4, z4.shape[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1., 2.],\n",
       "           [2., 4.]]]], grad_fn=<MkldnnConvolutionBackward>),\n",
       " torch.Size([1, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv5 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 2, stride = 3, padding = 1)\n",
    "conv5.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv5.state_dict()['bias'][0] = 0.0\n",
    "conv.state_dict()\n",
    "z5 = conv5(image1)\n",
    "z5, z4.shape[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3767, -0.0040,  1.0667,  0.2048],\n",
       "          [-0.0287, -0.9756, -0.0347, -0.9754],\n",
       "          [-1.7278, -0.2694,  0.8425,  0.3524],\n",
       "          [ 1.1588, -0.0125,  0.9309,  0.2482]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.randn((1, 1, 4, 4))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3)\n",
    "conv.state_dict()['weight'][0][0] = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0.0, 0]])\n",
    "conv.state_dict()['bias'][0] = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
