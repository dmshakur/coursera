{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png\" width=\"400px\" align=\"center\"></a>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">RECURRENT NETWORKS and LSTM IN DEEP LEARNING</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Applying Recurrent Neural Networks/LSTM for Language Modeling</h2>\n",
    "Hello and welcome to this part. In this notebook, we will go over the topic of Language Modelling, and create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and benchmark on the Penn Treebank dataset. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Objective</h2>\n",
    "By now, you should have an understanding of how Recurrent Networks work -- a specialized model to process sequential data by keeping track of the \"state\" or context. In this notebook, we go over a TensorFlow code snippet for creating a model focused on <b>Language Modelling</b> -- a very relevant task that is the cornerstone of many different linguistic problems such as <b>Speech Recognition, Machine Translation and Image Captioning</b>. For this, we will be using the Penn Treebank dataset, which is an often-used dataset for benchmarking Language Modelling models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Table of Contents</h2>\n",
    "<ol>\n",
    "    <li><a href=\"#language_modelling\">What exactly is Language Modelling?</a></li>\n",
    "    <li><a href=\"#treebank_dataset\">The Penn Treebank dataset</a></li>\n",
    "    <li><a href=\"#word_embedding\">Work Embedding</a></li>\n",
    "    <li><a href=\"#building_lstm_model\">Building the LSTM model for Language Modeling</a></li>\n",
    "    <li><a href=\"#ltsm\">LTSM</a></li>\n",
    "</ol>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"language_modelling\"></a>\n",
    "<h2>What exactly is Language Modelling?</h2>\n",
    "Language Modelling, to put it simply, <b>is the task of assigning probabilities to sequences of words</b>. This means that, given a context of one or a sequence of words in the language the model was trained on, the model should provide the next most probable words or sequence of words that follows from the given sequence of words the sentence. Language Modelling is one of the most important tasks in Natural Language Processing.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png\" width=\"1080\">\n",
    "<center><i>Example of a sentence being predicted</i></center>\n",
    "<br><br>\n",
    "In this example, one can see the predictions for the next word of a sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as <b>Speech Recognition, Machine Translation, Image Captioning, Text Correction</b> and many other very relevant problems. \n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png\" width=\"1080\">\n",
    "<center><i>The above example is a schema of an RNN in execution</i></center>\n",
    "<br><br>\n",
    "As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach <b>low levels of perplexity</b> on our desired dataset.\n",
    "\n",
    "For Language Modelling problems, <b>perplexity</b> is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that <b>low perplexity means a higher degree of trust in the predictions the model makes</b>. Therefore, the lower perplexity is, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"treebank_dataset\"></a>\n",
    "<h2>The Penn Treebank dataset</h2>\n",
    "Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be able to be correct at all. This means that we need a <b>large amount of data, annotated by or at least corrected by humans</b>. This is, of course, not an easy task at all.\n",
    "\n",
    "The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is <i>huge</i> -- there are over <b>four million and eight hundred thousand</b> annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank is commonly used as a benchmark dataset for Language Modelling.\n",
    "\n",
    "The dataset is divided in different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag --<code>&lt;unk&gt;</code>\n",
    ", which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence.\n",
    "\n",
    "<center>Example of text from the dataset we are going to use, <b>ptb.train</b></center>\n",
    "<br><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <center>the percentage of lung cancer deaths among the workers at the west <code>&lt;unk&gt;</code> mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said \n",
    " the plant which is owned by <code>&lt;unk&gt;</code> & <code>&lt;unk&gt;</code> co. was under contract with <code>&lt;unk&gt;</code> to make the cigarette filters \n",
    " the finding probably will support those who argue that the U.S. should regulate the class of asbestos including <code>&lt;unk&gt;</code> more <code>&lt;unk&gt;</code> than the common kind of asbestos <code>&lt;unk&gt;</code> found in most schools and other buildings dr. <code>&lt;unk&gt;</code> said</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"word_embedding\"></a>\n",
    "<h2>Word Embeddings</h2><br/>\n",
    "\n",
    "For better processing, in this example, we will make use of <a href=\"https://www.tensorflow.org/tutorials/word2vec/\"><b>word embeddings</b></a>, which is <b>a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers</b>. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be a very effective task in Natural Language Processing, and is a commonplace practice.\n",
    "<br><br>\n",
    "<font size=\"4\"><strong>\n",
    "$$Vec(\"Example\") = [0.02, 0.00, 0.00, 0.92, 0.30, \\ldots]$$\n",
    "</strong></font>\n",
    "<br>\n",
    "Word Embedding tends to group up similarly used words <i>reasonably</i> close together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and plot these words in a 2-dimensional space, we might see something like this:\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png\" width=\"800\">\n",
    "<center><i>T-SNE Mockup with clusters marked for easier visualization</i></center>\n",
    "<br><br>\n",
    "As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher they are correlated. For example, \"None\" is pretty semantically close to \"Zero\", while a phrase that uses \"Italy\", you could probably also fit \"Germany\" in it, with little damage to the sentence structure. The vectorial \"closeness\" for similar words like this is a great indicator of a well-built model.\n",
    "\n",
    "<hr>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow.models.rnn</code></b> model, which includes the function for building RNNs, and <b><code>tensorflow.models.rnn.ptb.reader</code></b> which is the helper module for getting the input data from the dataset we just downloaded.\n",
    "\n",
    "If you want to learn more take a look at https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data already exists.\n",
      "SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\n",
      "syswgetrc = c:/progra~1/wget/etc/wgetrc\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "!unzip -o data/ptb.zip -d data\n",
    "!cp data/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"building_lstm_model\"></a>\n",
    "<h2>Building the LSTM model for Language Modeling</h2>\n",
    "Now that we know exactly what we are doing, we can start building our model using TensorFlow. The very first thing we need to do is download and extract the <code>simple-examples</code> dataset, which can be done by executing the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\n",
      "syswgetrc = c:/progra~1/wget/etc/wgetrc\n",
      "--2020-05-23 12:41:41--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz... 147.229.9.23\n",
      "Connecting to www.fit.vutbr.cz|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: `simple-examples.tgz.2'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  135K 4m13s\n",
      "    50K .......... .......... .......... .......... ..........  0%  144K 4m4s\n",
      "   100K .......... .......... .......... .......... ..........  0%  277K 3m23s\n",
      "   150K .......... .......... .......... .......... ..........  0%  272K 3m3s\n",
      "   200K .......... .......... .......... .......... ..........  0%  292K 2m50s\n",
      "   250K .......... .......... .......... .......... ..........  0% 2.67M 2m23s\n",
      "   300K .......... .......... .......... .......... ..........  1%  285K 2m19s\n",
      "   350K .......... .......... .......... .......... ..........  1% 4.17M 2m3s\n",
      "   400K .......... .......... .......... .......... ..........  1%  281K 2m2s\n",
      "   450K .......... .......... .......... .......... ..........  1% 4.72M 1m51s\n",
      "   500K .......... .......... .......... .......... ..........  1%  283K 1m51s\n",
      "   550K .......... .......... .......... .......... ..........  1% 2.17M 1m43s\n",
      "   600K .......... .......... .......... .......... ..........  1% 4.39M 95s\n",
      "   650K .......... .......... .......... .......... ..........  2%  270K 97s\n",
      "   700K .......... .......... .......... .......... ..........  2% 1.01M 93s\n",
      "   750K .......... .......... .......... .......... ..........  2%  496K 91s\n",
      "   800K .......... .......... .......... .......... ..........  2%  551K 89s\n",
      "   850K .......... .......... .......... .......... ..........  2% 4.46M 85s\n",
      "   900K .......... .......... .......... .......... ..........  2%  579K 83s\n",
      "   950K .......... .......... .......... .......... ..........  2%  564K 82s\n",
      "  1000K .......... .......... .......... .......... ..........  3% 3.35M 78s\n",
      "  1050K .......... .......... .......... .......... ..........  3%  651K 77s\n",
      "  1100K .......... .......... .......... .......... ..........  3%  490K 76s\n",
      "  1150K .......... .......... .......... .......... ..........  3% 3.11M 73s\n",
      "  1200K .......... .......... .......... .......... ..........  3% 4.25M 71s\n",
      "  1250K .......... .......... .......... .......... ..........  3%  299K 72s\n",
      "  1300K .......... .......... .......... .......... ..........  3% 2.21M 70s\n",
      "  1350K .......... .......... .......... .......... ..........  4% 10.6M 67s\n",
      "  1400K .......... .......... .......... .......... ..........  4% 1.97M 65s\n",
      "  1450K .......... .......... .......... .......... ..........  4%  338K 66s\n",
      "  1500K .......... .......... .......... .......... ..........  4% 3.06M 64s\n",
      "  1550K .......... .......... .......... .......... ..........  4% 1.26M 63s\n",
      "  1600K .......... .......... .......... .......... ..........  4% 8.57M 61s\n",
      "  1650K .......... .......... .......... .......... ..........  4%  682K 61s\n",
      "  1700K .......... .......... .......... .......... ..........  5%  457K 61s\n",
      "  1750K .......... .......... .......... .......... ..........  5% 4.77M 59s\n",
      "  1800K .......... .......... .......... .......... ..........  5% 3.07M 58s\n",
      "  1850K .......... .......... .......... .......... ..........  5% 4.40M 56s\n",
      "  1900K .......... .......... .......... .......... ..........  5%  521K 57s\n",
      "  1950K .......... .......... .......... .......... ..........  5%  643K 56s\n",
      "  2000K .......... .......... .......... .......... ..........  6% 10.9M 55s\n",
      "  2050K .......... .......... .......... .......... ..........  6% 1.90M 54s\n",
      "  2100K .......... .......... .......... .......... ..........  6% 6.17M 53s\n",
      "  2150K .......... .......... .......... .......... ..........  6%  745K 52s\n",
      "  2200K .......... .......... .......... .......... ..........  6%  604K 52s\n",
      "  2250K .......... .......... .......... .......... ..........  6% 1.15M 52s\n",
      "  2300K .......... .......... .......... .......... ..........  6% 3.10M 51s\n",
      "  2350K .......... .......... .......... .......... ..........  7% 3.21M 50s\n",
      "  2400K .......... .......... .......... .......... ..........  7% 7.95M 49s\n",
      "  2450K .......... .......... .......... .......... ..........  7%  514K 49s\n",
      "  2500K .......... .......... .......... .......... ..........  7%  756K 49s\n",
      "  2550K .......... .......... .......... .......... ..........  7% 1.72M 48s\n",
      "  2600K .......... .......... .......... .......... ..........  7% 2.52M 47s\n",
      "  2650K .......... .......... .......... .......... ..........  7% 3.31M 46s\n",
      "  2700K .......... .......... .......... .......... ..........  8% 2.37M 46s\n",
      "  2750K .......... .......... .......... .......... ..........  8% 3.36M 45s\n",
      "  2800K .......... .......... .......... .......... ..........  8% 1.41M 45s\n",
      "  2850K .......... .......... .......... .......... ..........  8%  334K 45s\n",
      "  2900K .......... .......... .......... .......... ..........  8% 2.16M 45s\n",
      "  2950K .......... .......... .......... .......... ..........  8%  190M 44s\n",
      "  3000K .......... .......... .......... .......... ..........  8% 1.84M 43s\n",
      "  3050K .......... .......... .......... .......... ..........  9% 8.73M 43s\n",
      "  3100K .......... .......... .......... .......... ..........  9% 1.49M 42s\n",
      "  3150K .......... .......... .......... .......... ..........  9%  272M 42s\n",
      "  3200K .......... .......... .......... .......... ..........  9%  442K 42s\n",
      "  3250K .......... .......... .......... .......... ..........  9% 1.95M 41s\n",
      "  3300K .......... .......... .......... .......... ..........  9% 9.75M 41s\n",
      "  3350K .......... .......... .......... .......... ..........  9% 2.10M 40s\n",
      "  3400K .......... .......... .......... .......... .......... 10% 6.87M 40s\n",
      "  3450K .......... .......... .......... .......... .......... 10% 3.74M 39s\n",
      "  3500K .......... .......... .......... .......... .......... 10% 2.38M 39s\n",
      "  3550K .......... .......... .......... .......... .......... 10% 1.90M 38s\n",
      "  3600K .......... .......... .......... .......... .......... 10%  543K 39s\n",
      "  3650K .......... .......... .......... .......... .......... 10% 1.52M 38s\n",
      "  3700K .......... .......... .......... .......... .......... 11% 3.78M 38s\n",
      "  3750K .......... .......... .......... .......... .......... 11% 1.90M 37s\n",
      "  3800K .......... .......... .......... .......... .......... 11% 2.44M 37s\n",
      "  3850K .......... .......... .......... .......... .......... 11% 7.41M 37s\n",
      "  3900K .......... .......... .......... .......... .......... 11% 1.99M 36s\n",
      "  3950K .......... .......... .......... .......... .......... 11% 18.8M 36s\n",
      "  4000K .......... .......... .......... .......... .......... 11%  770K 36s\n",
      "  4050K .......... .......... .......... .......... .......... 12% 1.80M 35s\n",
      "  4100K .......... .......... .......... .......... .......... 12% 1.53M 35s\n",
      "  4150K .......... .......... .......... .......... .......... 12% 1.56M 35s\n",
      "  4200K .......... .......... .......... .......... .......... 12% 4.48M 35s\n",
      "  4250K .......... .......... .......... .......... .......... 12% 2.13M 34s\n",
      "  4300K .......... .......... .......... .......... .......... 12% 1.72M 34s\n",
      "  4350K .......... .......... .......... .......... .......... 12% 2.97M 34s\n",
      "  4400K .......... .......... .......... .......... .......... 13% 2.92M 33s\n",
      "  4450K .......... .......... .......... .......... .......... 13% 2.89M 33s\n",
      "  4500K .......... .......... .......... .......... .......... 13% 2.45M 33s\n",
      "  4550K .......... .......... .......... .......... .......... 13% 2.06M 32s\n",
      "  4600K .......... .......... .......... .......... .......... 13% 1.12M 32s\n",
      "  4650K .......... .......... .......... .......... .......... 13% 2.80M 32s\n",
      "  4700K .......... .......... .......... .......... .......... 13%  968K 32s\n",
      "  4750K .......... .......... .......... .......... .......... 14% 2.83M 32s\n",
      "  4800K .......... .......... .......... .......... .......... 14% 2.92M 31s\n",
      "  4850K .......... .......... .......... .......... .......... 14% 2.32M 31s\n",
      "  4900K .......... .......... .......... .......... .......... 14% 2.34M 31s\n",
      "  4950K .......... .......... .......... .......... .......... 14% 3.28M 31s\n",
      "  5000K .......... .......... .......... .......... .......... 14% 2.55M 30s\n",
      "  5050K .......... .......... .......... .......... .......... 14% 2.28M 30s\n",
      "  5100K .......... .......... .......... .......... .......... 15% 2.79M 30s\n",
      "  5150K .......... .......... .......... .......... .......... 15%  930K 30s\n",
      "  5200K .......... .......... .......... .......... .......... 15% 2.96M 30s\n",
      "  5250K .......... .......... .......... .......... .......... 15%  491K 30s\n",
      "  5300K .......... .......... .......... .......... .......... 15% 4.95M 30s\n",
      "  5350K .......... .......... .......... .......... .......... 15% 2.41M 29s\n",
      "  5400K .......... .......... .......... .......... .......... 16% 1.54M 29s\n",
      "  5450K .......... .......... .......... .......... .......... 16% 9.39M 29s\n",
      "  5500K .......... .......... .......... .......... .......... 16% 2.13M 29s\n",
      "  5550K .......... .......... .......... .......... .......... 16% 2.15M 29s\n",
      "  5600K .......... .......... .......... .......... .......... 16% 1.52M 28s\n",
      "  5650K .......... .......... .......... .......... .......... 16% 3.90M 28s\n",
      "  5700K .......... .......... .......... .......... .......... 16%  976K 28s\n",
      "  5750K .......... .......... .......... .......... .......... 17% 2.80M 28s\n",
      "  5800K .......... .......... .......... .......... .......... 17%  428K 28s\n",
      "  5850K .......... .......... .......... .......... .......... 17% 3.09M 28s\n",
      "  5900K .......... .......... .......... .......... .......... 17% 3.18M 28s\n",
      "  5950K .......... .......... .......... .......... .......... 17% 2.52M 28s\n",
      "  6000K .......... .......... .......... .......... .......... 17% 3.09M 27s\n",
      "  6050K .......... .......... .......... .......... .......... 17% 1.33M 27s\n",
      "  6100K .......... .......... .......... .......... .......... 18% 7.39M 27s\n",
      "  6150K .......... .......... .......... .......... .......... 18% 2.79M 27s\n",
      "  6200K .......... .......... .......... .......... .......... 18% 2.64M 27s\n",
      "  6250K .......... .......... .......... .......... .......... 18%  485K 27s\n",
      "  6300K .......... .......... .......... .......... .......... 18% 2.82M 27s\n",
      "  6350K .......... .......... .......... .......... .......... 18% 3.09M 26s\n",
      "  6400K .......... .......... .......... .......... .......... 18% 2.49M 26s\n",
      "  6450K .......... .......... .......... .......... .......... 19% 3.01M 26s\n",
      "  6500K .......... .......... .......... .......... .......... 19% 3.01M 26s\n",
      "  6550K .......... .......... .......... .......... .......... 19% 1.62M 26s\n",
      "  6600K .......... .......... .......... .......... .......... 19% 4.03M 26s\n",
      "  6650K .......... .......... .......... .......... .......... 19% 2.42M 25s\n",
      "  6700K .......... .......... .......... .......... .......... 19% 1.37M 25s\n",
      "  6750K .......... .......... .......... .......... .......... 19%  565K 26s\n",
      "  6800K .......... .......... .......... .......... .......... 20% 3.20M 25s\n",
      "  6850K .......... .......... .......... .......... .......... 20% 2.48M 25s\n",
      "  6900K .......... .......... .......... .......... .......... 20% 3.23M 25s\n",
      "  6950K .......... .......... .......... .......... .......... 20% 1.93M 25s\n",
      "  7000K .......... .......... .......... .......... .......... 20% 2.28M 25s\n",
      "  7050K .......... .......... .......... .......... .......... 20% 3.06M 25s\n",
      "  7100K .......... .......... .......... .......... .......... 20% 2.42M 24s\n",
      "  7150K .......... .......... .......... .......... .......... 21% 2.88M 24s\n",
      "  7200K .......... .......... .......... .......... .......... 21%  628K 24s\n",
      "  7250K .......... .......... .......... .......... .......... 21% 2.84M 24s\n",
      "  7300K .......... .......... .......... .......... .......... 21% 1.84M 24s\n",
      "  7350K .......... .......... .......... .......... .......... 21% 5.39M 24s\n",
      "  7400K .......... .......... .......... .......... .......... 21% 3.03M 24s\n",
      "  7450K .......... .......... .......... .......... .......... 22% 2.09M 24s\n",
      "  7500K .......... .......... .......... .......... .......... 22% 3.15M 24s\n",
      "  7550K .......... .......... .......... .......... .......... 22% 3.21M 23s\n",
      "  7600K .......... .......... .......... .......... .......... 22% 2.39M 23s\n",
      "  7650K .......... .......... .......... .......... .......... 22%  490K 23s\n",
      "  7700K .......... .......... .......... .......... .......... 22% 2.25M 23s\n",
      "  7750K .......... .......... .......... .......... .......... 22% 2.73M 23s\n",
      "  7800K .......... .......... .......... .......... .......... 23% 3.28M 23s\n",
      "  7850K .......... .......... .......... .......... .......... 23% 1.61M 23s\n",
      "  7900K .......... .......... .......... .......... .......... 23% 5.39M 23s\n",
      "  7950K .......... .......... .......... .......... .......... 23% 1.80M 23s\n",
      "  8000K .......... .......... .......... .......... .......... 23% 3.23M 23s\n",
      "  8050K .......... .......... .......... .......... .......... 23% 2.25M 22s\n",
      "  8100K .......... .......... .......... .......... .......... 23% 1.65M 22s\n",
      "  8150K .......... .......... .......... .......... .......... 24%  442K 23s\n",
      "  8200K .......... .......... .......... .......... .......... 24% 1.91M 22s\n",
      "  8250K .......... .......... .......... .......... .......... 24% 2.93M 22s\n",
      "  8300K .......... .......... .......... .......... .......... 24% 3.24M 22s\n",
      "  8350K .......... .......... .......... .......... .......... 24% 2.37M 22s\n",
      "  8400K .......... .......... .......... .......... .......... 24% 3.06M 22s\n",
      "  8450K .......... .......... .......... .......... .......... 24% 2.37M 22s\n",
      "  8500K .......... .......... .......... .......... .......... 25% 2.50M 22s\n",
      "  8550K .......... .......... .......... .......... .......... 25% 2.35M 22s\n",
      "  8600K .......... .......... .......... .......... .......... 25%  620K 22s\n",
      "  8650K .......... .......... .......... .......... .......... 25% 2.75M 22s\n",
      "  8700K .......... .......... .......... .......... .......... 25%  764K 22s\n",
      "  8750K .......... .......... .......... .......... .......... 25% 2.95M 21s\n",
      "  8800K .......... .......... .......... .......... .......... 25% 2.94M 21s\n",
      "  8850K .......... .......... .......... .......... .......... 26% 2.42M 21s\n",
      "  8900K .......... .......... .......... .......... .......... 26% 3.15M 21s\n",
      "  8950K .......... .......... .......... .......... .......... 26% 2.29M 21s\n",
      "  9000K .......... .......... .......... .......... .......... 26% 2.77M 21s\n",
      "  9050K .......... .......... .......... .......... .......... 26% 1.78M 21s\n",
      "  9100K .......... .......... .......... .......... .......... 26% 4.87M 21s\n",
      "  9150K .......... .......... .......... .......... .......... 27% 1.17M 21s\n",
      "  9200K .......... .......... .......... .......... .......... 27%  759K 21s\n",
      "  9250K .......... .......... .......... .......... .......... 27% 3.25M 21s\n",
      "  9300K .......... .......... .......... .......... .......... 27% 1.90M 20s\n",
      "  9350K .......... .......... .......... .......... .......... 27% 1.94M 20s\n",
      "  9400K .......... .......... .......... .......... .......... 27% 2.24M 20s\n",
      "  9450K .......... .......... .......... .......... .......... 27% 3.34M 20s\n",
      "  9500K .......... .......... .......... .......... .......... 28% 2.37M 20s\n",
      "  9550K .......... .......... .......... .......... .......... 28% 3.25M 20s\n",
      "  9600K .......... .......... .......... .......... .......... 28% 2.69M 20s\n",
      "  9650K .......... .......... .......... .......... .......... 28%  637K 20s\n",
      "  9700K .......... .......... .......... .......... .......... 28% 3.00M 20s\n",
      "  9750K .......... .......... .......... .......... .......... 28% 3.24M 20s\n",
      "  9800K .......... .......... .......... .......... .......... 28% 2.62M 20s\n",
      "  9850K .......... .......... .......... .......... .......... 29% 2.34M 20s\n",
      "  9900K .......... .......... .......... .......... .......... 29% 3.27M 19s\n",
      "  9950K .......... .......... .......... .......... .......... 29% 2.31M 19s\n",
      " 10000K .......... .......... .......... .......... .......... 29% 2.91M 19s\n",
      " 10050K .......... .......... .......... .......... .......... 29% 2.42M 19s\n",
      " 10100K .......... .......... .......... .......... .......... 29%  608K 19s\n",
      " 10150K .......... .......... .......... .......... .......... 29% 2.75M 19s\n",
      " 10200K .......... .......... .......... .......... .......... 30% 1.38M 19s\n",
      " 10250K .......... .......... .......... .......... .......... 30% 5.02M 19s\n",
      " 10300K .......... .......... .......... .......... .......... 30% 2.34M 19s\n",
      " 10350K .......... .......... .......... .......... .......... 30% 3.30M 19s\n",
      " 10400K .......... .......... .......... .......... .......... 30% 3.04M 19s\n",
      " 10450K .......... .......... .......... .......... .......... 30% 1.24M 19s\n",
      " 10500K .......... .......... .......... .......... .......... 30% 19.1M 19s\n",
      " 10550K .......... .......... .......... .......... .......... 31%  803K 19s\n",
      " 10600K .......... .......... .......... .......... .......... 31% 2.70M 18s\n",
      " 10650K .......... .......... .......... .......... .......... 31% 2.38M 18s\n",
      " 10700K .......... .......... .......... .......... .......... 31% 2.11M 18s\n",
      " 10750K .......... .......... .......... .......... .......... 31% 2.88M 18s\n",
      " 10800K .......... .......... .......... .......... .......... 31% 2.41M 18s\n",
      " 10850K .......... .......... .......... .......... .......... 32% 1.36M 18s\n",
      " 10900K .......... .......... .......... .......... .......... 32% 2.38M 18s\n",
      " 10950K .......... .......... .......... .......... .......... 32% 3.36M 18s\n",
      " 11000K .......... .......... .......... .......... .......... 32%  886K 18s\n",
      " 11050K .......... .......... .......... .......... .......... 32% 2.83M 18s\n",
      " 11100K .......... .......... .......... .......... .......... 32% 2.05M 18s\n",
      " 11150K .......... .......... .......... .......... .......... 32% 2.83M 18s\n",
      " 11200K .......... .......... .......... .......... .......... 33% 2.98M 18s\n",
      " 11250K .......... .......... .......... .......... .......... 33% 2.79M 18s\n",
      " 11300K .......... .......... .......... .......... .......... 33% 2.58M 17s\n",
      " 11350K .......... .......... .......... .......... .......... 33% 2.43M 17s\n",
      " 11400K .......... .......... .......... .......... .......... 33% 2.59M 17s\n",
      " 11450K .......... .......... .......... .......... .......... 33%  947K 17s\n",
      " 11500K .......... .......... .......... .......... .......... 33% 1.44M 17s\n",
      " 11550K .......... .......... .......... .......... .......... 34% 1.05M 17s\n",
      " 11600K .......... .......... .......... .......... .......... 34% 2.87M 17s\n",
      " 11650K .......... .......... .......... .......... .......... 34% 2.18M 17s\n",
      " 11700K .......... .......... .......... .......... .......... 34% 2.33M 17s\n",
      " 11750K .......... .......... .......... .......... .......... 34% 3.20M 17s\n",
      " 11800K .......... .......... .......... .......... .......... 34% 2.15M 17s\n",
      " 11850K .......... .......... .......... .......... .......... 34%  813K 17s\n",
      " 11900K .......... .......... .......... .......... .......... 35% 4.31M 17s\n",
      " 11950K .......... .......... .......... .......... .......... 35% 2.37M 17s\n",
      " 12000K .......... .......... .......... .......... .......... 35% 3.54M 17s\n",
      " 12050K .......... .......... .......... .......... .......... 35% 2.86M 17s\n",
      " 12100K .......... .......... .......... .......... .......... 35% 1.60M 17s\n",
      " 12150K .......... .......... .......... .......... .......... 35% 1.56M 16s\n",
      " 12200K .......... .......... .......... .......... .......... 35% 1.90M 16s\n",
      " 12250K .......... .......... .......... .......... .......... 36% 3.15M 16s\n",
      " 12300K .......... .......... .......... .......... .......... 36%  692K 16s\n",
      " 12350K .......... .......... .......... .......... .......... 36% 4.48M 16s\n",
      " 12400K .......... .......... .......... .......... .......... 36% 2.43M 16s\n",
      " 12450K .......... .......... .......... .......... .......... 36% 3.04M 16s\n",
      " 12500K .......... .......... .......... .......... .......... 36% 3.29M 16s\n",
      " 12550K .......... .......... .......... .......... .......... 37% 2.47M 16s\n",
      " 12600K .......... .......... .......... .......... .......... 37% 1.37M 16s\n",
      " 12650K .......... .......... .......... .......... .......... 37% 2.27M 16s\n",
      " 12700K .......... .......... .......... .......... .......... 37% 3.08M 16s\n",
      " 12750K .......... .......... .......... .......... .......... 37%  893K 16s\n",
      " 12800K .......... .......... .......... .......... .......... 37%  759K 16s\n",
      " 12850K .......... .......... .......... .......... .......... 37% 2.34M 16s\n",
      " 12900K .......... .......... .......... .......... .......... 38% 3.23M 16s\n",
      " 12950K .......... .......... .......... .......... .......... 38% 1.35M 16s\n",
      " 13000K .......... .......... .......... .......... .......... 38% 3.72M 16s\n",
      " 13050K .......... .......... .......... .......... .......... 38% 4.95M 15s\n",
      " 13100K .......... .......... .......... .......... .......... 38% 1.89M 15s\n",
      " 13150K .......... .......... .......... .......... .......... 38% 4.17M 15s\n",
      " 13200K .......... .......... .......... .......... .......... 38% 2.53M 15s\n",
      " 13250K .......... .......... .......... .......... .......... 39%  862K 15s\n",
      " 13300K .......... .......... .......... .......... .......... 39% 2.21M 15s\n",
      " 13350K .......... .......... .......... .......... .......... 39%  421K 15s\n",
      " 13400K .......... .......... .......... .......... .......... 39% 2.74M 15s\n",
      " 13450K .......... .......... .......... .......... .......... 39% 2.42M 15s\n",
      " 13500K .......... .......... .......... .......... .......... 39% 3.11M 15s\n",
      " 13550K .......... .......... .......... .......... .......... 39% 2.40M 15s\n",
      " 13600K .......... .......... .......... .......... .......... 40% 3.28M 15s\n",
      " 13650K .......... .......... .......... .......... .......... 40% 3.16M 15s\n",
      " 13700K .......... .......... .......... .......... .......... 40% 2.24M 15s\n",
      " 13750K .......... .......... .......... .......... .......... 40% 3.22M 15s\n",
      " 13800K .......... .......... .......... .......... .......... 40%  554K 15s\n",
      " 13850K .......... .......... .......... .......... .......... 40% 3.88M 15s\n",
      " 13900K .......... .......... .......... .......... .......... 40% 1.92M 15s\n",
      " 13950K .......... .......... .......... .......... .......... 41% 4.98M 15s\n",
      " 14000K .......... .......... .......... .......... .......... 41% 1.56M 15s\n",
      " 14050K .......... .......... .......... .......... .......... 41% 2.53M 15s\n",
      " 14100K .......... .......... .......... .......... .......... 41% 3.19M 14s\n",
      " 14150K .......... .......... .......... .......... .......... 41% 2.26M 14s\n",
      " 14200K .......... .......... .......... .......... .......... 41% 3.15M 14s\n",
      " 14250K .......... .......... .......... .......... .......... 41%  867K 14s\n",
      " 14300K .......... .......... .......... .......... .......... 42% 3.43M 14s\n",
      " 14350K .......... .......... .......... .......... .......... 42% 1.61M 14s\n",
      " 14400K .......... .......... .......... .......... .......... 42%  847K 14s\n",
      " 14450K .......... .......... .......... .......... .......... 42% 2.36M 14s\n",
      " 14500K .......... .......... .......... .......... .......... 42% 2.72M 14s\n",
      " 14550K .......... .......... .......... .......... .......... 42% 3.35M 14s\n",
      " 14600K .......... .......... .......... .......... .......... 43% 1.82M 14s\n",
      " 14650K .......... .......... .......... .......... .......... 43% 4.71M 14s\n",
      " 14700K .......... .......... .......... .......... .......... 43% 2.48M 14s\n",
      " 14750K .......... .......... .......... .......... .......... 43% 3.14M 14s\n",
      " 14800K .......... .......... .......... .......... .......... 43% 3.03M 14s\n",
      " 14850K .......... .......... .......... .......... .......... 43%  933K 14s\n",
      " 14900K .......... .......... .......... .......... .......... 43% 1.38M 14s\n",
      " 14950K .......... .......... .......... .......... .......... 44% 1.37M 14s\n",
      " 15000K .......... .......... .......... .......... .......... 44% 1.70M 14s\n",
      " 15050K .......... .......... .......... .......... .......... 44% 5.53M 14s\n",
      " 15100K .......... .......... .......... .......... .......... 44%  897K 14s\n",
      " 15150K .......... .......... .......... .......... .......... 44% 8.33M 13s\n",
      " 15200K .......... .......... .......... .......... .......... 44% 1.70M 13s\n",
      " 15250K .......... .......... .......... .......... .......... 44% 3.13M 13s\n",
      " 15300K .......... .......... .......... .......... .......... 45%  722K 13s\n",
      " 15350K .......... .......... .......... .......... .......... 45% 1.43M 13s\n",
      " 15400K .......... .......... .......... .......... .......... 45% 1.26M 13s\n",
      " 15450K .......... .......... .......... .......... .......... 45% 1.23M 13s\n",
      " 15500K .......... .......... .......... .......... .......... 45% 1.27M 13s\n",
      " 15550K .......... .......... .......... .......... .......... 45% 2.44M 13s\n",
      " 15600K .......... .......... .......... .......... .......... 45% 3.36M 13s\n",
      " 15650K .......... .......... .......... .......... .......... 46% 1.56M 13s\n",
      " 15700K .......... .......... .......... .......... .......... 46% 1.41M 13s\n",
      " 15750K .......... .......... .......... .......... .......... 46% 1.63M 13s\n",
      " 15800K .......... .......... .......... .......... .......... 46% 1.29M 13s\n",
      " 15850K .......... .......... .......... .......... .......... 46% 1.50M 13s\n",
      " 15900K .......... .......... .......... .......... .......... 46% 1.32M 13s\n",
      " 15950K .......... .......... .......... .......... .......... 46% 1.37M 13s\n",
      " 16000K .......... .......... .......... .......... .......... 47% 2.22M 13s\n",
      " 16050K .......... .......... .......... .......... .......... 47% 3.26M 13s\n",
      " 16100K .......... .......... .......... .......... .......... 47% 2.04M 13s\n",
      " 16150K .......... .......... .......... .......... .......... 47%  247K 13s\n",
      " 16200K .......... .......... .......... .......... .......... 47% 2.28M 13s\n",
      " 16250K .......... .......... .......... .......... .......... 47% 2.35M 13s\n",
      " 16300K .......... .......... .......... .......... .......... 48% 2.43M 13s\n",
      " 16350K .......... .......... .......... .......... .......... 48% 3.30M 13s\n",
      " 16400K .......... .......... .......... .......... .......... 48% 2.27M 13s\n",
      " 16450K .......... .......... .......... .......... .......... 48% 1.60M 13s\n",
      " 16500K .......... .......... .......... .......... .......... 48% 3.27M 12s\n",
      " 16550K .......... .......... .......... .......... .......... 48% 4.58M 12s\n",
      " 16600K .......... .......... .......... .......... .......... 48% 1.40M 12s\n",
      " 16650K .......... .......... .......... .......... .......... 49%  606K 12s\n",
      " 16700K .......... .......... .......... .......... .......... 49% 2.88M 12s\n",
      " 16750K .......... .......... .......... .......... .......... 49% 2.41M 12s\n",
      " 16800K .......... .......... .......... .......... .......... 49% 2.42M 12s\n",
      " 16850K .......... .......... .......... .......... .......... 49% 2.12M 12s\n",
      " 16900K .......... .......... .......... .......... .......... 49% 2.55M 12s\n",
      " 16950K .......... .......... .......... .......... .......... 49% 3.34M 12s\n",
      " 17000K .......... .......... .......... .......... .......... 50% 2.43M 12s\n",
      " 17050K .......... .......... .......... .......... .......... 50% 2.13M 12s\n",
      " 17100K .......... .......... .......... .......... .......... 50% 1.78M 12s\n",
      " 17150K .......... .......... .......... .......... .......... 50%  783K 12s\n",
      " 17200K .......... .......... .......... .......... .......... 50% 2.64M 12s\n",
      " 17250K .......... .......... .......... .......... .......... 50% 2.12M 12s\n",
      " 17300K .......... .......... .......... .......... .......... 50% 3.18M 12s\n",
      " 17350K .......... .......... .......... .......... .......... 51% 2.04M 12s\n",
      " 17400K .......... .......... .......... .......... .......... 51% 1.50M 12s\n",
      " 17450K .......... .......... .......... .......... .......... 51% 1.03M 12s\n",
      " 17500K .......... .......... .......... .......... .......... 51% 4.77M 12s\n",
      " 17550K .......... .......... .......... .......... .......... 51% 2.29M 12s\n",
      " 17600K .......... .......... .......... .......... .......... 51% 1.52M 12s\n",
      " 17650K .......... .......... .......... .......... .......... 51% 3.30M 11s\n",
      " 17700K .......... .......... .......... .......... .......... 52% 2.17M 11s\n",
      " 17750K .......... .......... .......... .......... .......... 52% 3.30M 11s\n",
      " 17800K .......... .......... .......... .......... .......... 52% 2.38M 11s\n",
      " 17850K .......... .......... .......... .......... .......... 52% 2.57M 11s\n",
      " 17900K .......... .......... .......... .......... .......... 52% 1.00M 11s\n",
      " 17950K .......... .......... .......... .......... .......... 52% 2.75M 11s\n",
      " 18000K .......... .......... .......... .......... .......... 53% 3.35M 11s\n",
      " 18050K .......... .......... .......... .......... .......... 53%  641K 11s\n",
      " 18100K .......... .......... .......... .......... .......... 53% 2.74M 11s\n",
      " 18150K .......... .......... .......... .......... .......... 53% 3.08M 11s\n",
      " 18200K .......... .......... .......... .......... .......... 53% 2.82M 11s\n",
      " 18250K .......... .......... .......... .......... .......... 53% 1.92M 11s\n",
      " 18300K .......... .......... .......... .......... .......... 53% 3.05M 11s\n",
      " 18350K .......... .......... .......... .......... .......... 54% 2.10M 11s\n",
      " 18400K .......... .......... .......... .......... .......... 54% 3.28M 11s\n",
      " 18450K .......... .......... .......... .......... .......... 54% 2.51M 11s\n",
      " 18500K .......... .......... .......... .......... .......... 54%  759K 11s\n",
      " 18550K .......... .......... .......... .......... .......... 54% 2.48M 11s\n",
      " 18600K .......... .......... .......... .......... .......... 54% 2.49M 11s\n",
      " 18650K .......... .......... .......... .......... .......... 54%  971K 11s\n",
      " 18700K .......... .......... .......... .......... .......... 55% 2.43M 11s\n",
      " 18750K .......... .......... .......... .......... .......... 55% 3.13M 11s\n",
      " 18800K .......... .......... .......... .......... .......... 55% 3.22M 10s\n",
      " 18850K .......... .......... .......... .......... .......... 55% 1.63M 10s\n",
      " 18900K .......... .......... .......... .......... .......... 55% 3.19M 10s\n",
      " 18950K .......... .......... .......... .......... .......... 55% 1.54M 10s\n",
      " 19000K .......... .......... .......... .......... .......... 55% 2.85M 10s\n",
      " 19050K .......... .......... .......... .......... .......... 56% 3.14M 10s\n",
      " 19100K .......... .......... .......... .......... .......... 56%  855K 10s\n",
      " 19150K .......... .......... .......... .......... .......... 56% 2.42M 10s\n",
      " 19200K .......... .......... .......... .......... .......... 56% 2.97M 10s\n",
      " 19250K .......... .......... .......... .......... .......... 56% 2.47M 10s\n",
      " 19300K .......... .......... .......... .......... .......... 56% 2.29M 10s\n",
      " 19350K .......... .......... .......... .......... .......... 56% 3.26M 10s\n",
      " 19400K .......... .......... .......... .......... .......... 57% 1.42M 10s\n",
      " 19450K .......... .......... .......... .......... .......... 57% 2.07M 10s\n",
      " 19500K .......... .......... .......... .......... .......... 57% 2.28M 10s\n",
      " 19550K .......... .......... .......... .......... .......... 57%  907K 10s\n",
      " 19600K .......... .......... .......... .......... .......... 57% 2.03M 10s\n",
      " 19650K .......... .......... .......... .......... .......... 57% 2.74M 10s\n",
      " 19700K .......... .......... .......... .......... .......... 57% 3.25M 10s\n",
      " 19750K .......... .......... .......... .......... .......... 58% 1.87M 10s\n",
      " 19800K .......... .......... .......... .......... .......... 58% 4.49M 10s\n",
      " 19850K .......... .......... .......... .......... .......... 58% 2.85M 10s\n",
      " 19900K .......... .......... .......... .......... .......... 58% 1.80M 10s\n",
      " 19950K .......... .......... .......... .......... .......... 58% 3.01M 10s\n",
      " 20000K .......... .......... .......... .......... .......... 58% 1.26M 9s\n",
      " 20050K .......... .......... .......... .......... .......... 59%  698K 9s\n",
      " 20100K .......... .......... .......... .......... .......... 59% 1.63M 9s\n",
      " 20150K .......... .......... .......... .......... .......... 59% 2.79M 9s\n",
      " 20200K .......... .......... .......... .......... .......... 59% 3.09M 9s\n",
      " 20250K .......... .......... .......... .......... .......... 59% 1.73M 9s\n",
      " 20300K .......... .......... .......... .......... .......... 59% 3.97M 9s\n",
      " 20350K .......... .......... .......... .......... .......... 59% 3.00M 9s\n",
      " 20400K .......... .......... .......... .......... .......... 60% 2.20M 9s\n",
      " 20450K .......... .......... .......... .......... .......... 60% 3.39M 9s\n",
      " 20500K .......... .......... .......... .......... .......... 60%  369K 9s\n",
      " 20550K .......... .......... .......... .......... .......... 60% 2.61M 9s\n",
      " 20600K .......... .......... .......... .......... .......... 60% 2.86M 9s\n",
      " 20650K .......... .......... .......... .......... .......... 60% 2.44M 9s\n",
      " 20700K .......... .......... .......... .......... .......... 60% 1.95M 9s\n",
      " 20750K .......... .......... .......... .......... .......... 61% 4.73M 9s\n",
      " 20800K .......... .......... .......... .......... .......... 61% 1.88M 9s\n",
      " 20850K .......... .......... .......... .......... .......... 61% 3.31M 9s\n",
      " 20900K .......... .......... .......... .......... .......... 61% 3.31M 9s\n",
      " 20950K .......... .......... .......... .......... .......... 61%  859K 9s\n",
      " 21000K .......... .......... .......... .......... .......... 61% 2.09M 9s\n",
      " 21050K .......... .......... .......... .......... .......... 61%  701K 9s\n",
      " 21100K .......... .......... .......... .......... .......... 62% 3.06M 9s\n",
      " 21150K .......... .......... .......... .......... .......... 62% 3.39M 9s\n",
      " 21200K .......... .......... .......... .......... .......... 62% 3.21M 9s\n",
      " 21250K .......... .......... .......... .......... .......... 62% 1.40M 9s\n",
      " 21300K .......... .......... .......... .......... .......... 62% 1.70M 9s\n",
      " 21350K .......... .......... .......... .......... .......... 62% 3.07M 8s\n",
      " 21400K .......... .......... .......... .......... .......... 62% 4.20M 8s\n",
      " 21450K .......... .......... .......... .......... .......... 63% 3.36M 8s\n",
      " 21500K .......... .......... .......... .......... .......... 63%  628K 8s\n",
      " 21550K .......... .......... .......... .......... .......... 63% 1.65M 8s\n",
      " 21600K .......... .......... .......... .......... .......... 63% 1.76M 8s\n",
      " 21650K .......... .......... .......... .......... .......... 63% 1.92M 8s\n",
      " 21700K .......... .......... .......... .......... .......... 63% 5.44M 8s\n",
      " 21750K .......... .......... .......... .......... .......... 64% 1.81M 8s\n",
      " 21800K .......... .......... .......... .......... .......... 64% 2.14M 8s\n",
      " 21850K .......... .......... .......... .......... .......... 64% 1.84M 8s\n",
      " 21900K .......... .......... .......... .......... .......... 64% 3.02M 8s\n",
      " 21950K .......... .......... .......... .......... .......... 64%  818K 8s\n",
      " 22000K .......... .......... .......... .......... .......... 64% 1.76M 8s\n",
      " 22050K .......... .......... .......... .......... .......... 64% 5.32M 8s\n",
      " 22100K .......... .......... .......... .......... .......... 65%  434K 8s\n",
      " 22150K .......... .......... .......... .......... .......... 65% 1.84M 8s\n",
      " 22200K .......... .......... .......... .......... .......... 65% 3.12M 8s\n",
      " 22250K .......... .......... .......... .......... .......... 65% 1.93M 8s\n",
      " 22300K .......... .......... .......... .......... .......... 65% 2.27M 8s\n",
      " 22350K .......... .......... .......... .......... .......... 65% 1.06M 8s\n",
      " 22400K .......... .......... .......... .......... .......... 65% 1.24M 8s\n",
      " 22450K .......... .......... .......... .......... .......... 66% 1.30M 8s\n",
      " 22500K .......... .......... .......... .......... .......... 66% 1.49M 8s\n",
      " 22550K .......... .......... .......... .......... .......... 66% 1.12M 8s\n",
      " 22600K .......... .......... .......... .......... .......... 66% 1.66M 8s\n",
      " 22650K .......... .......... .......... .......... .......... 66% 1.91M 8s\n",
      " 22700K .......... .......... .......... .......... .......... 66% 2.12M 8s\n",
      " 22750K .......... .......... .......... .......... .......... 66% 3.39M 8s\n",
      " 22800K .......... .......... .......... .......... .......... 67% 2.42M 7s\n",
      " 22850K .......... .......... .......... .......... .......... 67% 2.47M 7s\n",
      " 22900K .......... .......... .......... .......... .......... 67% 1.61M 7s\n",
      " 22950K .......... .......... .......... .......... .......... 67% 4.45M 7s\n",
      " 23000K .......... .......... .......... .......... .......... 67% 1.04M 7s\n",
      " 23050K .......... .......... .......... .......... .......... 67% 3.57M 7s\n",
      " 23100K .......... .......... .......... .......... .......... 67% 1.87M 7s\n",
      " 23150K .......... .......... .......... .......... .......... 68% 1.55M 7s\n",
      " 23200K .......... .......... .......... .......... .......... 68% 2.30M 7s\n",
      " 23250K .......... .......... .......... .......... .......... 68% 2.04M 7s\n",
      " 23300K .......... .......... .......... .......... .......... 68% 5.25M 7s\n",
      " 23350K .......... .......... .......... .......... .......... 68% 2.87M 7s\n",
      " 23400K .......... .......... .......... .......... .......... 68% 2.73M 7s\n",
      " 23450K .......... .......... .......... .......... .......... 69% 1.66M 7s\n",
      " 23500K .......... .......... .......... .......... .......... 69% 2.50M 7s\n",
      " 23550K .......... .......... .......... .......... .......... 69% 2.02M 7s\n",
      " 23600K .......... .......... .......... .......... .......... 69%  870K 7s\n",
      " 23650K .......... .......... .......... .......... .......... 69% 2.55M 7s\n",
      " 23700K .......... .......... .......... .......... .......... 69% 1.34M 7s\n",
      " 23750K .......... .......... .......... .......... .......... 69% 3.16M 7s\n",
      " 23800K .......... .......... .......... .......... .......... 70% 1.81M 7s\n",
      " 23850K .......... .......... .......... .......... .......... 70% 2.82M 7s\n",
      " 23900K .......... .......... .......... .......... .......... 70% 5.11M 7s\n",
      " 23950K .......... .......... .......... .......... .......... 70% 3.18M 7s\n",
      " 24000K .......... .......... .......... .......... .......... 70% 2.39M 7s\n",
      " 24050K .......... .......... .......... .......... .......... 70% 1.44M 7s\n",
      " 24100K .......... .......... .......... .......... .......... 70% 2.40M 7s\n",
      " 24150K .......... .......... .......... .......... .......... 71% 1.06M 6s\n",
      " 24200K .......... .......... .......... .......... .......... 71% 1.96M 6s\n",
      " 24250K .......... .......... .......... .......... .......... 71% 2.62M 6s\n",
      " 24300K .......... .......... .......... .......... .......... 71% 2.21M 6s\n",
      " 24350K .......... .......... .......... .......... .......... 71% 2.51M 6s\n",
      " 24400K .......... .......... .......... .......... .......... 71% 2.39M 6s\n",
      " 24450K .......... .......... .......... .......... .......... 71% 2.69M 6s\n",
      " 24500K .......... .......... .......... .......... .......... 72% 1.78M 6s\n",
      " 24550K .......... .......... .......... .......... .......... 72% 3.31M 6s\n",
      " 24600K .......... .......... .......... .......... .......... 72% 2.34M 6s\n",
      " 24650K .......... .......... .......... .......... .......... 72% 1.48M 6s\n",
      " 24700K .......... .......... .......... .......... .......... 72%  988K 6s\n",
      " 24750K .......... .......... .......... .......... .......... 72% 1.41M 6s\n",
      " 24800K .......... .......... .......... .......... .......... 72%  625K 6s\n",
      " 24850K .......... .......... .......... .......... .......... 73% 1.27M 6s\n",
      " 24900K .......... .......... .......... .......... .......... 73% 1.32M 6s\n",
      " 24950K .......... .......... .......... .......... .......... 73% 1.47M 6s\n",
      " 25000K .......... .......... .......... .......... .......... 73% 2.21M 6s\n",
      " 25050K .......... .......... .......... .......... .......... 73% 3.01M 6s\n",
      " 25100K .......... .......... .......... .......... .......... 73% 1.65M 6s\n",
      " 25150K .......... .......... .......... .......... .......... 74% 4.25M 6s\n",
      " 25200K .......... .......... .......... .......... .......... 74% 2.29M 6s\n",
      " 25250K .......... .......... .......... .......... .......... 74% 1.44M 6s\n",
      " 25300K .......... .......... .......... .......... .......... 74% 1.70M 6s\n",
      " 25350K .......... .......... .......... .......... .......... 74% 2.45M 6s\n",
      " 25400K .......... .......... .......... .......... .......... 74% 2.18M 6s\n",
      " 25450K .......... .......... .......... .......... .......... 74% 2.94M 6s\n",
      " 25500K .......... .......... .......... .......... .......... 75% 1.71M 6s\n",
      " 25550K .......... .......... .......... .......... .......... 75% 2.63M 6s\n",
      " 25600K .......... .......... .......... .......... .......... 75% 2.15M 5s\n",
      " 25650K .......... .......... .......... .......... .......... 75% 3.42M 5s\n",
      " 25700K .......... .......... .......... .......... .......... 75% 1.16M 5s\n",
      " 25750K .......... .......... .......... .......... .......... 75% 1.67M 5s\n",
      " 25800K .......... .......... .......... .......... .......... 75% 5.40M 5s\n",
      " 25850K .......... .......... .......... .......... .......... 76% 2.57M 5s\n",
      " 25900K .......... .......... .......... .......... .......... 76% 3.20M 5s\n",
      " 25950K .......... .......... .......... .......... .......... 76% 2.18M 5s\n",
      " 26000K .......... .......... .......... .......... .......... 76% 2.27M 5s\n",
      " 26050K .......... .......... .......... .......... .......... 76% 1.71M 5s\n",
      " 26100K .......... .......... .......... .......... .......... 76% 2.45M 5s\n",
      " 26150K .......... .......... .......... .......... .......... 76%  768K 5s\n",
      " 26200K .......... .......... .......... .......... .......... 77% 2.36M 5s\n",
      " 26250K .......... .......... .......... .......... .......... 77% 2.81M 5s\n",
      " 26300K .......... .......... .......... .......... .......... 77% 3.28M 5s\n",
      " 26350K .......... .......... .......... .......... .......... 77% 2.04M 5s\n",
      " 26400K .......... .......... .......... .......... .......... 77% 1.26M 5s\n",
      " 26450K .......... .......... .......... .......... .......... 77% 5.36M 5s\n",
      " 26500K .......... .......... .......... .......... .......... 77% 2.39M 5s\n",
      " 26550K .......... .......... .......... .......... .......... 78% 3.00M 5s\n",
      " 26600K .......... .......... .......... .......... .......... 78% 1.54M 5s\n",
      " 26650K .......... .......... .......... .......... .......... 78%  955K 5s\n",
      " 26700K .......... .......... .......... .......... .......... 78% 2.41M 5s\n",
      " 26750K .......... .......... .......... .......... .......... 78% 2.77M 5s\n",
      " 26800K .......... .......... .......... .......... .......... 78% 2.46M 5s\n",
      " 26850K .......... .......... .......... .......... .......... 78% 1.73M 5s\n",
      " 26900K .......... .......... .......... .......... .......... 79% 2.30M 5s\n",
      " 26950K .......... .......... .......... .......... .......... 79% 1.87M 5s\n",
      " 27000K .......... .......... .......... .......... .......... 79% 3.86M 5s\n",
      " 27050K .......... .......... .......... .......... .......... 79% 2.50M 4s\n",
      " 27100K .......... .......... .......... .......... .......... 79% 1.15M 4s\n",
      " 27150K .......... .......... .......... .......... .......... 79% 2.17M 4s\n",
      " 27200K .......... .......... .......... .......... .......... 80% 4.56M 4s\n",
      " 27250K .......... .......... .......... .......... .......... 80% 2.27M 4s\n",
      " 27300K .......... .......... .......... .......... .......... 80% 1.54M 4s\n",
      " 27350K .......... .......... .......... .......... .......... 80%  995K 4s\n",
      " 27400K .......... .......... .......... .......... .......... 80% 1.53M 4s\n",
      " 27450K .......... .......... .......... .......... .......... 80% 5.04M 4s\n",
      " 27500K .......... .......... .......... .......... .......... 80% 2.91M 4s\n",
      " 27550K .......... .......... .......... .......... .......... 81% 2.60M 4s\n",
      " 27600K .......... .......... .......... .......... .......... 81% 1.88M 4s\n",
      " 27650K .......... .......... .......... .......... .......... 81% 1.19M 4s\n",
      " 27700K .......... .......... .......... .......... .......... 81% 1.33M 4s\n",
      " 27750K .......... .......... .......... .......... .......... 81% 1.48M 4s\n",
      " 27800K .......... .......... .......... .......... .......... 81% 1.25M 4s\n",
      " 27850K .......... .......... .......... .......... .......... 81% 1.93M 4s\n",
      " 27900K .......... .......... .......... .......... .......... 82% 2.09M 4s\n",
      " 27950K .......... .......... .......... .......... .......... 82% 2.11M 4s\n",
      " 28000K .......... .......... .......... .......... .......... 82% 1.96M 4s\n",
      " 28050K .......... .......... .......... .......... .......... 82% 1.46M 4s\n",
      " 28100K .......... .......... .......... .......... .......... 82% 2.30M 4s\n",
      " 28150K .......... .......... .......... .......... .......... 82% 1.25M 4s\n",
      " 28200K .......... .......... .......... .......... .......... 82% 3.00M 4s\n",
      " 28250K .......... .......... .......... .......... .......... 83% 2.35M 4s\n",
      " 28300K .......... .......... .......... .......... .......... 83% 2.45M 4s\n",
      " 28350K .......... .......... .......... .......... .......... 83% 2.78M 4s\n",
      " 28400K .......... .......... .......... .......... .......... 83% 2.97M 4s\n",
      " 28450K .......... .......... .......... .......... .......... 83% 2.88M 4s\n",
      " 28500K .......... .......... .......... .......... .......... 83% 1.40M 4s\n",
      " 28550K .......... .......... .......... .......... .......... 83%  893K 3s\n",
      " 28600K .......... .......... .......... .......... .......... 84% 1.46M 3s\n",
      " 28650K .......... .......... .......... .......... .......... 84% 3.26M 3s\n",
      " 28700K .......... .......... .......... .......... .......... 84% 2.24M 3s\n",
      " 28750K .......... .......... .......... .......... .......... 84% 3.23M 3s\n",
      " 28800K .......... .......... .......... .......... .......... 84% 1.62M 3s\n",
      " 28850K .......... .......... .......... .......... .......... 84% 2.22M 3s\n",
      " 28900K .......... .......... .......... .......... .......... 85% 3.25M 3s\n",
      " 28950K .......... .......... .......... .......... .......... 85% 2.09M 3s\n",
      " 29000K .......... .......... .......... .......... .......... 85% 2.88M 3s\n",
      " 29050K .......... .......... .......... .......... .......... 85%  813K 3s\n",
      " 29100K .......... .......... .......... .......... .......... 85% 1.36M 3s\n",
      " 29150K .......... .......... .......... .......... .......... 85% 2.96M 3s\n",
      " 29200K .......... .......... .......... .......... .......... 85% 1.68M 3s\n",
      " 29250K .......... .......... .......... .......... .......... 86% 1.04M 3s\n",
      " 29300K .......... .......... .......... .......... .......... 86% 1.66M 3s\n",
      " 29350K .......... .......... .......... .......... .......... 86% 5.12M 3s\n",
      " 29400K .......... .......... .......... .......... .......... 86% 2.37M 3s\n",
      " 29450K .......... .......... .......... .......... .......... 86% 3.16M 3s\n",
      " 29500K .......... .......... .......... .......... .......... 86% 1.38M 3s\n",
      " 29550K .......... .......... .......... .......... .......... 86% 1.47M 3s\n",
      " 29600K .......... .......... .......... .......... .......... 87% 2.34M 3s\n",
      " 29650K .......... .......... .......... .......... .......... 87% 3.35M 3s\n",
      " 29700K .......... .......... .......... .......... .......... 87% 1.31M 3s\n",
      " 29750K .......... .......... .......... .......... .......... 87% 1.06M 3s\n",
      " 29800K .......... .......... .......... .......... .......... 87% 2.28M 3s\n",
      " 29850K .......... .......... .......... .......... .......... 87% 1.63M 3s\n",
      " 29900K .......... .......... .......... .......... .......... 87% 3.20M 3s\n",
      " 29950K .......... .......... .......... .......... .......... 88% 2.03M 3s\n",
      " 30000K .......... .......... .......... .......... .......... 88% 1.85M 3s\n",
      " 30050K .......... .......... .......... .......... .......... 88% 3.37M 2s\n",
      " 30100K .......... .......... .......... .......... .......... 88% 1.86M 2s\n",
      " 30150K .......... .......... .......... .......... .......... 88% 1.98M 2s\n",
      " 30200K .......... .......... .......... .......... .......... 88%  699K 2s\n",
      " 30250K .......... .......... .......... .......... .......... 88% 1.50M 2s\n",
      " 30300K .......... .......... .......... .......... .......... 89% 1.94M 2s\n",
      " 30350K .......... .......... .......... .......... .......... 89% 2.93M 2s\n",
      " 30400K .......... .......... .......... .......... .......... 89% 1.95M 2s\n",
      " 30450K .......... .......... .......... .......... .......... 89% 4.87M 2s\n",
      " 30500K .......... .......... .......... .......... .......... 89% 2.58M 2s\n",
      " 30550K .......... .......... .......... .......... .......... 89% 1.45M 2s\n",
      " 30600K .......... .......... .......... .......... .......... 90% 2.28M 2s\n",
      " 30650K .......... .......... .......... .......... .......... 90% 1.85M 2s\n",
      " 30700K .......... .......... .......... .......... .......... 90% 1.52M 2s\n",
      " 30750K .......... .......... .......... .......... .......... 90% 1.59M 2s\n",
      " 30800K .......... .......... .......... .......... .......... 90% 1.54M 2s\n",
      " 30850K .......... .......... .......... .......... .......... 90% 1.49M 2s\n",
      " 30900K .......... .......... .......... .......... .......... 90% 2.40M 2s\n",
      " 30950K .......... .......... .......... .......... .......... 91% 2.05M 2s\n",
      " 31000K .......... .......... .......... .......... .......... 91% 4.89M 2s\n",
      " 31050K .......... .......... .......... .......... .......... 91% 1.71M 2s\n",
      " 31100K .......... .......... .......... .......... .......... 91% 1.70M 2s\n",
      " 31150K .......... .......... .......... .......... .......... 91% 2.19M 2s\n",
      " 31200K .......... .......... .......... .......... .......... 91% 3.04M 2s\n",
      " 31250K .......... .......... .......... .......... .......... 91% 1.90M 2s\n",
      " 31300K .......... .......... .......... .......... .......... 92%  979K 2s\n",
      " 31350K .......... .......... .......... .......... .......... 92% 4.34M 2s\n",
      " 31400K .......... .......... .......... .......... .......... 92% 1.29M 2s\n",
      " 31450K .......... .......... .......... .......... .......... 92% 3.31M 2s\n",
      " 31500K .......... .......... .......... .......... .......... 92% 1.62M 2s\n",
      " 31550K .......... .......... .......... .......... .......... 92% 2.29M 2s\n",
      " 31600K .......... .......... .......... .......... .......... 92% 1.82M 2s\n",
      " 31650K .......... .......... .......... .......... .......... 93% 2.51M 1s\n",
      " 31700K .......... .......... .......... .......... .......... 93% 2.28M 1s\n",
      " 31750K .......... .......... .......... .......... .......... 93% 2.30M 1s\n",
      " 31800K .......... .......... .......... .......... .......... 93% 2.68M 1s\n",
      " 31850K .......... .......... .......... .......... .......... 93% 2.20M 1s\n",
      " 31900K .......... .......... .......... .......... .......... 93% 2.77M 1s\n",
      " 31950K .......... .......... .......... .......... .......... 93% 1.56M 1s\n",
      " 32000K .......... .......... .......... .......... .......... 94% 1.32M 1s\n",
      " 32050K .......... .......... .......... .......... .......... 94% 2.03M 1s\n",
      " 32100K .......... .......... .......... .......... .......... 94% 1.42M 1s\n",
      " 32150K .......... .......... .......... .......... .......... 94% 1.54M 1s\n",
      " 32200K .......... .......... .......... .......... .......... 94% 3.20M 1s\n",
      " 32250K .......... .......... .......... .......... .......... 94% 4.78M 1s\n",
      " 32300K .......... .......... .......... .......... .......... 95% 2.25M 1s\n",
      " 32350K .......... .......... .......... .......... .......... 95% 2.78M 1s\n",
      " 32400K .......... .......... .......... .......... .......... 95% 3.65M 1s\n",
      " 32450K .......... .......... .......... .......... .......... 95% 1.02M 1s\n",
      " 32500K .......... .......... .......... .......... .......... 95% 2.94M 1s\n",
      " 32550K .......... .......... .......... .......... .......... 95%  923K 1s\n",
      " 32600K .......... .......... .......... .......... .......... 95% 1.88M 1s\n",
      " 32650K .......... .......... .......... .......... .......... 96% 2.97M 1s\n",
      " 32700K .......... .......... .......... .......... .......... 96% 2.20M 1s\n",
      " 32750K .......... .......... .......... .......... .......... 96% 3.29M 1s\n",
      " 32800K .......... .......... .......... .......... .......... 96% 2.19M 1s\n",
      " 32850K .......... .......... .......... .......... .......... 96% 1.24M 1s\n",
      " 32900K .......... .......... .......... .......... .......... 96% 1.69M 1s\n",
      " 32950K .......... .......... .......... .......... .......... 96% 1.46M 1s\n",
      " 33000K .......... .......... .......... .......... .......... 97% 1.32M 1s\n",
      " 33050K .......... .......... .......... .......... .......... 97% 1.44M 1s\n",
      " 33100K .......... .......... .......... .......... .......... 97% 1.67M 1s\n",
      " 33150K .......... .......... .......... .......... .......... 97% 2.30M 1s\n",
      " 33200K .......... .......... .......... .......... .......... 97% 3.16M 0s\n",
      " 33250K .......... .......... .......... .......... .......... 97% 1.96M 0s\n",
      " 33300K .......... .......... .......... .......... .......... 97% 2.57M 0s\n",
      " 33350K .......... .......... .......... .......... .......... 98% 3.06M 0s\n",
      " 33400K .......... .......... .......... .......... .......... 98% 1.90M 0s\n",
      " 33450K .......... .......... .......... .......... .......... 98% 2.38M 0s\n",
      " 33500K .......... .......... .......... .......... .......... 98% 1.27M 0s\n",
      " 33550K .......... .......... .......... .......... .......... 98%  766K 0s\n",
      " 33600K .......... .......... .......... .......... .......... 98% 2.72M 0s\n",
      " 33650K .......... .......... .......... .......... .......... 98% 3.10M 0s\n",
      " 33700K .......... .......... .......... .......... .......... 99% 3.03M 0s\n",
      " 33750K .......... .......... .......... .......... .......... 99% 2.27M 0s\n",
      " 33800K .......... .......... .......... .......... .......... 99% 2.84M 0s\n",
      " 33850K .......... .......... .......... .......... .......... 99% 1.87M 0s\n",
      " 33900K .......... .......... .......... .......... .......... 99% 3.47M 0s\n",
      " 33950K .......... .......... .......... .......... .......... 99% 2.74M 0s\n",
      " 34000K .......... .......... .......... .......... .......... 99% 1.32M 0s\n",
      " 34050K ..                                                    100% 45.9G=21s\n",
      "\n",
      "2020-05-23 12:42:03 (1.58 MB/s) - `simple-examples.tgz.2' saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Additionally, for the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change those!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 60\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Some clarifications for LSTM architecture based on the arguments:\n",
    "\n",
    "Network structure:\n",
    "<ul>\n",
    "    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "    </li>\n",
    "    <li>The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n",
    "    <li>the structure is like:\n",
    "        <ul>\n",
    "            <li>200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Input layer: \n",
    "<ul>\n",
    "    <li>The network has 200 input units.</li>\n",
    "    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n",
    "    </li>\n",
    "    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Hidden layer:\n",
    "<ul>\n",
    "    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to apply what you just learned about language modelling to the code you see, you should be able to understand it.\n",
    "\n",
    "This code is adapted from the <a href=\"https://github.com/tensorflow/models\">PTBModel</a> example bundled with the TensorFlow source code.\n",
    "\n",
    "\n",
    "<h4>Train data</h4>\n",
    "The story starts from data:\n",
    "<ul>\n",
    "    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>\n",
    "    <li>We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$floor(\\frac{N}{b \\times h})+1=1548$$ iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, andh is size of each sentence. So, the number of iterators is 1548\n",
    "    </li>\n",
    "    <li>Each batch data is read from train dataset of size 600, and shape of [30x20]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First we start an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Courses\\coursera\\ibm_ai_engineer\\5_building_deep_learning_model_with_tensorflow\\week_3\\reader.py:30: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets just read one mini-batch now and feed our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets look at 3 sentences of our input x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n",
       "           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "we define 2 place holders to feed them with mini-batchs, that is x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define a dictionary, and use it later to feed the placeholders with our first mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict = {_input_data:x, _targets:y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For example, we can use it to feed <code>\\_input\\_data</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, ..., 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361, ...,  241,   13, 2420],\n",
       "       [2654,    6,  334, ...,  514,    8,  605],\n",
       "       ...,\n",
       "       [7831,   36, 1678, ...,    4, 4558,  157],\n",
       "       [  59, 2070, 2433, ...,  400,    1, 1173],\n",
       "       [2097,    3,    2, ..., 2043,   23,    1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-15-ddf565d0e753>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-15-ddf565d0e753>:3: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, we initialize the states of the nework:\n",
    "\n",
    "<h4>_initial_state</h4>\n",
    "\n",
    "For each LCTM, there are 2 state matrices, c\\_state and m\\_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size [30x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets look at the states, though they are all zero for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Embeddings</h3>\n",
    "We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, One-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n",
    "\n",
    "The embedded vectors also get updated during the training process of the deep neural network.\n",
    "We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of [10000x200] for all 10000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the <code>embedding_words</code> variable with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.85824335e-02, -2.59509310e-03,  1.68012492e-02, ...,\n",
       "        -9.40492377e-03, -1.65884383e-03, -7.00323842e-03],\n",
       "       [ 7.16917217e-03, -1.54643012e-02,  2.27620341e-02, ...,\n",
       "         1.02333128e-02,  2.19038315e-02,  9.41471383e-03],\n",
       "       [ 2.06207372e-02, -7.81391561e-03,  5.33573888e-03, ...,\n",
       "        -2.32895464e-03,  1.85612589e-02,  2.03288160e-02],\n",
       "       ...,\n",
       "       [-1.55720180e-02,  2.34860368e-03,  2.12859958e-02, ...,\n",
       "         2.09334232e-02, -1.26452930e-03,  1.83724239e-02],\n",
       "       [-1.19981896e-02, -1.81602072e-02, -1.38538927e-02, ...,\n",
       "        -5.58990240e-03, -7.05812126e-05,  1.28171295e-02],\n",
       "       [-1.33317616e-02, -2.04884447e-03,  7.00373948e-03, ...,\n",
       "        -1.92446113e-02,  1.74321197e-02,  2.14671269e-02]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\n",
    "It creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(60, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #shape=(30, 20, 200) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01250438,  0.0107468 ,  0.00033174, ..., -0.01314829,\n",
       "        -0.00835587,  0.01769862],\n",
       "       [-0.00186938, -0.00478982, -0.01551156, ..., -0.00654577,\n",
       "        -0.01649625,  0.00734551],\n",
       "       [ 0.00345471, -0.00421824, -0.01575848, ..., -0.0103229 ,\n",
       "        -0.00619026,  0.00190011],\n",
       "       ...,\n",
       "       [-0.01673138, -0.01258661, -0.02038767, ...,  0.02258276,\n",
       "        -0.02076919, -0.01145899],\n",
       "       [-0.02148322,  0.00896325,  0.02263114, ..., -0.01257757,\n",
       "        -0.00990519,  0.02172451],\n",
       "       [-0.00908671, -0.00698438, -0.02396993, ...,  0.00810939,\n",
       "        -0.01814439, -0.02101704]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Constructing Recurrent Neural Networks</h3>\n",
    "<b>tf.nn.dynamic_rnn()</b> creates a recurrent neural network using <b>stacked_lstm</b>. \n",
    "\n",
    "The input should be a Tensor of shape: [batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)\n",
    "\n",
    "This method, returns a pair (outputs, new_state) where:\n",
    "<ul>\n",
    "    <li><b>outputs</b>: is a length T list of outputs (one for each input), or a nested tuple of such elements.</li>\n",
    "    <li><b>new_state</b>: is the final state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-f5b68f99f9ac>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Source-Files\\anaconda3\\envs\\old_tensorflow_1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Source-Files\\anaconda3\\envs\\old_tensorflow_1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "so, lets look at the outputs. The output of the stackedLSTM comes from 200 hidden_layer, and in each time step(=20), one of them get activated. we use the linear activation to map the 200 hidden layer to a [?x10 matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0002854 ,  0.00029721, -0.00038166, ...,  0.00060706,\n",
       "        -0.00028253, -0.0001634 ],\n",
       "       [ 0.00011793,  0.00077285, -0.00022347, ...,  0.00014254,\n",
       "        -0.00047688, -0.0007813 ],\n",
       "       [ 0.00019104,  0.00069513,  0.0004622 , ..., -0.0002467 ,\n",
       "        -0.00043585, -0.00079183],\n",
       "       ...,\n",
       "       [-0.00028546, -0.00075078, -0.00080071, ..., -0.00065256,\n",
       "        -0.00036635, -0.00030435],\n",
       "       [ 0.00012108, -0.00012425, -0.00047437, ..., -0.00104761,\n",
       "        -0.00059409, -0.00026686],\n",
       "       [ 0.00061548, -0.00041126, -0.00010454, ..., -0.00128066,\n",
       "        -0.0003146 , -0.00067074]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "we need to flatten the outputs to be able to connect it softmax layer. Lets reshape the output tensor from  [30 x 20 x 200] to [600 x 200].\n",
    "\n",
    "<b>Notice:</b> Imagine our output is 3-d tensor as following (of course each <code>sen_x_word_y</code> is a an embedded vector by itself): \n",
    "<ul>\n",
    "    <li>sentence 1: [[sen1word1], [sen1word2], [sen1word3], ..., [sen1word20]]</li> \n",
    "    <li>sentence 2: [[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20]]</li>   \n",
    "    <li>sentence 3: [[sen3word1], [sen3word2], [sen3word3], ..., [sen3word20]]</li>  \n",
    "    <li>...  </li>\n",
    "    <li>sentence 30: [[sen30word1], [sen30word2], [sen30word3], ..., [sen30word20]]</li>   \n",
    "</ul>\n",
    "Now, the flatten would convert this 3-dim tensor to:\n",
    "\n",
    "[ [sen1word1], [sen1word2], [sen1word3], ..., [sen1word20],[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20], ..., [sen30word20] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>logistic unit</h3>\n",
    "Now, we create a logistic unit to return the probability of the output word in our vocabulary with 1000 words. \n",
    "\n",
    "$$Softmax = [600 \\times 200] * [200 \\times 1000] + [1 \\times 1000] \\Longrightarrow [600 \\times 1000]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the probability of observing words for t=0 to t=20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output:  (1200, 10000)\n",
      "The probability of observing words in t=0 to t=20 [[9.98713222e-05 1.00125719e-04 1.01756850e-04 ... 9.98821342e-05\n",
      "  9.94252841e-05 1.01461665e-04]\n",
      " [9.98777978e-05 1.00124722e-04 1.01770092e-04 ... 9.98932883e-05\n",
      "  9.94247894e-05 1.01466343e-04]\n",
      " [9.98740434e-05 1.00125304e-04 1.01761827e-04 ... 9.98900068e-05\n",
      "  9.94172078e-05 1.01461388e-04]\n",
      " ...\n",
      " [9.98717514e-05 1.00123427e-04 1.01759200e-04 ... 9.98826945e-05\n",
      "  9.94223374e-05 1.01443562e-04]\n",
      " [9.98628093e-05 1.00117533e-04 1.01750389e-04 ... 9.98830656e-05\n",
      "  9.94149741e-05 1.01451609e-04]\n",
      " [9.98495962e-05 1.00123310e-04 1.01739919e-04 ... 9.98721443e-05\n",
      "  9.94140501e-05 1.01454643e-04]]\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "output_words_prob = session.run(prob, feed_dict)\n",
    "print(\"shape of the output: \", output_words_prob.shape)\n",
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Prediction</h3>\n",
    "What is the word correspond to the probability output? Lets use the maximum probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3997,    2,  242, 2507, 5566, 2385, 5566, 2026, 8146, 7287, 9134,\n",
       "       5218, 5218, 1151, 1189, 1189, 1189, 1189, 1189, 1189], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0:20], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So, what is the ground truth for the first word of first sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, you can get it from target tensor, if you want to find the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = session.run(_targets, feed_dict) \n",
    "targ[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar the predicted words are to the target words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Objective function</h4>\n",
    "\n",
    "Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "This function is already implemented and available in TensorFlow through <b>sequence_loss_by_example</b>. It calculates the weighted cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n",
    "\n",
    "The arguments of this function are:  \n",
    "<ul>\n",
    "    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n",
    "    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n",
    "    <li>weights: List of 1D batch-sized float-Tensors of the same length as logits.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "loss is a 1D batch-sized float Tensor [600x1]: The log-perplexity for each sequence. Lets look at the first 10 values of loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.208332, 9.218008, 9.217305, 9.217496, 9.220674, 9.208928,\n",
       "       9.207031, 9.200068, 9.197832, 9.20078 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define loss as average of the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.25142"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Training</h3>\n",
    "\n",
    "To do training for our network, we have to take the following steps:\n",
    "<ol>\n",
    "    <li>Define the optimizer.</li>\n",
    "    <li>Extract variables that are trainable.</li>\n",
    "    <li>Calculate the gradients based on the loss function.</li>\n",
    "    <li>Apply the optimizer to the variables/gradients tuple.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>1. Define Optimizer</h4>\n",
    "\n",
    "<b>GradientDescentOptimizer</b> constructs a new gradient descent optimizer. Later, we use constructed <b>optimizer</b> to compute gradients for a loss and apply gradients to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "<h4>2. Trainable Variables</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Note: we can find the name and scope of all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>3. Calculate the gradients based on the loss function</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Gradient</h4>:\n",
    "The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.\n",
    "\n",
    "First lets recall the gradient function using an toy example:\n",
    "$$ z = \\left(2x^2 + 3xy\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = tf.placeholder(tf.float32)\n",
    "var_y = tf.placeholder(tf.float32) \n",
    "func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(func_test, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The <b>tf.gradients()</b> function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensorsincluding variables. <b>tf.gradients(func, xs)</b> constructs symbolic partial derivatives of sum of <b>func</b> w.r.t. <i>x</i> in <b>xs</b>. \n",
    "\n",
    "Now, lets look at the derivitive w.r.t. <b>var_x</b>:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 4x + 3y $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_x])\n",
    "session.run(var_grad, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "the derivative w.r.t. <b>var_y</b>:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 3x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_y])\n",
    "session.run(var_grad, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we can look at gradients w.r.t all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x15d4b358e08>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/add_grad/Reshape:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "#sess.run(grad_t_list,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "now, we have a list of tensors, t-list. We can use it to find clipped tensors. <b>clip_by_global_norm</b> clips values of multiple tensors by the ratio of the sum of their norms.\n",
    "\n",
    "<b>clip_by_global_norm</b> get <i>t-list</i> as input and returns 2 things:\n",
    "<ul>\n",
    "    <li>a list of clipped tensors, so called <i>list_clipped</i></li> \n",
    "    <li>the global norm (global_norm) of all tensors in t_list</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Source-Files\\anaconda3\\envs\\old_tensorflow_1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x15d4c1eb888>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedSlicesValue(values=array([[ 2.8916811e-06, -1.1731065e-05, -9.4487523e-06, ...,\n",
       "         -6.7649075e-06, -1.3139665e-05, -3.0893802e-06],\n",
       "        [ 1.0505257e-05, -7.4301643e-06, -5.3370977e-06, ...,\n",
       "          1.9829680e-07, -2.6276328e-05, -7.8912317e-06],\n",
       "        [ 7.5594194e-06,  6.5036575e-06, -5.9879198e-07, ...,\n",
       "         -1.1885332e-05, -2.8630629e-05, -5.1714019e-06],\n",
       "        ...,\n",
       "        [ 8.8666739e-06,  2.1386238e-06, -4.2252154e-06, ...,\n",
       "         -1.0912116e-05, -2.2366648e-06,  1.2972055e-05],\n",
       "        [ 1.0449079e-05,  4.2535780e-06, -2.2201397e-07, ...,\n",
       "         -6.6918597e-06, -1.8062656e-06,  6.3975708e-06],\n",
       "        [ 2.0834771e-06,  5.3729432e-06,  2.5530719e-06, ...,\n",
       "         -4.9666205e-06, -1.0128132e-06,  2.8530721e-06]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1]), dense_shape=array([10000,   200])),\n",
       " array([[-2.0111806e-08, -2.8546218e-08, -3.7514251e-08, ...,\n",
       "          4.2320085e-08,  2.7115095e-09,  1.0529041e-08],\n",
       "        [-3.8670024e-08,  3.0869753e-08,  2.4665606e-08, ...,\n",
       "         -1.0900496e-09,  7.5377820e-09, -4.7972282e-08],\n",
       "        [ 7.4816295e-09,  1.9212244e-08, -6.3568230e-08, ...,\n",
       "          1.8742586e-08, -3.9753942e-09,  2.2494901e-09],\n",
       "        ...,\n",
       "        [ 9.0513730e-10, -5.3749449e-11,  4.2330872e-09, ...,\n",
       "          7.7328011e-09, -5.2259321e-09, -6.8887951e-10],\n",
       "        [-2.0217039e-09,  9.9736275e-10,  3.0554095e-09, ...,\n",
       "         -3.0972094e-09,  3.0367757e-09,  2.9783942e-10],\n",
       "        [ 3.8054866e-09, -5.2740801e-10, -4.1171746e-09, ...,\n",
       "         -6.0558500e-09, -5.6206195e-09,  5.0150297e-09]], dtype=float32),\n",
       " array([ 3.1443051e-07,  5.7494731e-06, -3.9021534e-06, ...,\n",
       "         2.6622834e-06,  2.0804002e-06, -1.8704824e-06], dtype=float32),\n",
       " array([[-3.4172514e-09, -3.4604029e-09, -5.6546163e-09, ...,\n",
       "         -2.3723263e-09,  4.8289922e-10, -5.3087068e-10],\n",
       "        [-2.0016648e-09, -6.6433752e-09, -3.5335530e-09, ...,\n",
       "         -1.4439793e-08,  5.0194400e-09, -8.0238287e-09],\n",
       "        [-6.4619745e-09, -9.8373985e-09, -1.0142841e-08, ...,\n",
       "          7.8376600e-10, -1.0489222e-08,  3.7757784e-09],\n",
       "        ...,\n",
       "        [ 9.4378727e-10, -7.0850770e-10,  2.5314235e-09, ...,\n",
       "         -2.4372275e-09, -2.0305282e-09,  3.6185746e-10],\n",
       "        [ 2.2094702e-10,  2.2960487e-09, -1.5975281e-09, ...,\n",
       "          1.5852385e-09, -3.8221808e-09, -1.1040020e-09],\n",
       "        [ 1.0552563e-09, -9.7350605e-10, -2.0727478e-09, ...,\n",
       "         -5.0779647e-10, -2.9436109e-09,  1.4642348e-09]], dtype=float32),\n",
       " array([-3.13848409e-06,  4.34803070e-07,  1.94988138e-06, -4.48517130e-06,\n",
       "        -1.92270500e-06, -2.76442023e-07, -1.01146850e-06, -3.10495761e-06,\n",
       "         3.11377039e-06,  1.74727847e-06,  1.96758106e-06,  5.02404237e-06,\n",
       "        -3.35890263e-06,  1.67445080e-06, -5.78922652e-08, -4.52727681e-06,\n",
       "         2.23543202e-06, -1.43715255e-07,  1.60518766e-06,  4.80972744e-07,\n",
       "        -5.22900427e-06,  5.22224900e-06,  1.60115485e-06,  6.68502054e-08,\n",
       "        -5.40966994e-06, -1.72151772e-07, -1.16992589e-06, -2.83899749e-06,\n",
       "         9.44276835e-08, -3.57971862e-06,  1.46278189e-05, -1.71477222e-06,\n",
       "         1.22011420e-06, -3.52481356e-06, -1.30681519e-06,  4.87959142e-06,\n",
       "         1.58702665e-06, -4.74874469e-06,  1.00377270e-06,  5.12545012e-06,\n",
       "         1.17360514e-06, -2.67430551e-06, -3.32030390e-06, -6.61508238e-06,\n",
       "        -2.60291699e-06,  2.69639531e-06,  4.80600647e-06,  1.80509005e-06,\n",
       "        -5.76083039e-06, -6.60057538e-08,  1.12167868e-06, -6.63806077e-06,\n",
       "         1.54087360e-07, -4.12976624e-06, -1.39068834e-06, -1.25873157e-06,\n",
       "         5.21272841e-06, -4.42964347e-06, -1.19585229e-06, -1.80892346e-06,\n",
       "        -1.18059336e-06, -3.99553846e-06,  2.73085584e-06,  3.27677685e-06,\n",
       "        -3.51604376e-06, -2.60478828e-06, -3.19872129e-07, -1.59022954e-06,\n",
       "        -3.18909474e-06, -2.48633705e-06, -2.51056872e-06,  3.80492042e-06,\n",
       "        -6.08442815e-06, -2.96092935e-07,  4.70372106e-06,  3.13157921e-06,\n",
       "        -6.73321483e-06,  3.24857865e-06,  1.36999358e-06,  6.83486951e-06,\n",
       "         3.16125579e-06, -9.55229780e-07,  7.44218823e-06,  8.92674450e-08,\n",
       "         6.21319896e-06,  3.77950687e-06,  2.16569035e-07,  3.54516578e-06,\n",
       "        -3.27852013e-06,  2.32440971e-06,  3.04321190e-07,  4.98399118e-08,\n",
       "         2.03208174e-07, -3.66882296e-06,  3.91783988e-06,  2.89606260e-06,\n",
       "         1.96728638e-06, -1.67817404e-06, -1.76143999e-07, -4.87640000e-06,\n",
       "        -3.02107628e-06, -8.57138446e-07, -3.72322916e-06, -1.64075504e-06,\n",
       "         1.63481764e-06,  2.20399193e-06,  6.18648210e-06, -2.81082703e-06,\n",
       "         1.21445005e-08,  2.17903221e-06, -1.64590392e-06, -1.18713956e-06,\n",
       "         5.98182567e-07,  3.80394749e-06, -7.72764679e-08, -2.29504417e-06,\n",
       "        -5.84562167e-06, -2.96565054e-06,  1.87150965e-06,  9.78119488e-07,\n",
       "         2.21859159e-06,  4.42050521e-08,  4.99115367e-06,  5.20437516e-06,\n",
       "         5.98376892e-06,  6.08104529e-06, -1.05440085e-06, -2.50748985e-06,\n",
       "        -1.82202980e-02, -4.37371014e-03, -2.17327708e-03,  1.86746917e-03,\n",
       "         6.10925490e-03,  6.69786381e-03,  1.57674085e-02, -7.31428945e-03,\n",
       "         3.54022020e-03,  1.89568684e-03, -2.42398400e-02,  2.09253095e-02,\n",
       "        -7.54267816e-03,  7.98702997e-04, -9.37662739e-03, -3.10931094e-02,\n",
       "         1.17868036e-02,  1.32585512e-02, -2.40313057e-02,  2.28430727e-04,\n",
       "         8.92329589e-03,  3.00251916e-02,  6.52616937e-03,  1.76329594e-02,\n",
       "        -1.65814757e-02,  7.26185367e-03, -8.95939302e-03, -1.52529143e-02,\n",
       "        -3.05646122e-03, -1.68160466e-03,  2.35841163e-02, -1.09292613e-02,\n",
       "         4.40049637e-03, -1.44133577e-02, -9.50796250e-03, -3.93446861e-03,\n",
       "         1.29025625e-02, -1.62665434e-02, -1.19492458e-02,  2.09156126e-02,\n",
       "         1.27078975e-02,  1.90131692e-03,  9.02389991e-04,  2.58000270e-02,\n",
       "        -1.90478843e-02, -2.81845536e-02,  1.55261634e-02, -5.13307098e-03,\n",
       "        -2.45025731e-04,  2.92802998e-03, -8.57114885e-03,  4.85684862e-03,\n",
       "        -5.09755639e-03,  2.63359980e-03, -1.32026244e-02,  7.26666860e-03,\n",
       "        -3.68171767e-03, -1.02118682e-02, -1.80577934e-02,  9.65429284e-03,\n",
       "        -1.62156951e-02, -2.70925020e-03, -7.21158739e-03, -2.83324299e-03,\n",
       "        -1.89508069e-02, -2.03106459e-02, -8.82705732e-04, -1.12044197e-02,\n",
       "         1.24494382e-03, -1.09173823e-02,  1.53215686e-02, -6.22039544e-04,\n",
       "        -1.72626320e-02, -2.56716143e-02, -7.03292154e-03, -1.03346966e-02,\n",
       "         5.53686684e-03,  2.73323767e-02, -4.13963478e-03,  3.61370035e-02,\n",
       "        -2.97180121e-03,  1.11200958e-02, -1.52800474e-02, -3.10633727e-03,\n",
       "         1.26112681e-02,  4.09537135e-03, -1.24570020e-02,  6.56417198e-03,\n",
       "        -1.43889748e-02,  1.41402269e-02,  5.66311786e-03,  1.66442543e-02,\n",
       "         2.46163718e-02,  1.33779487e-02, -2.88818777e-02, -1.32040987e-02,\n",
       "        -4.58618207e-03, -3.03437207e-02, -1.99539997e-02, -6.58347388e-04,\n",
       "        -3.24770762e-03, -3.31602022e-02, -2.96974182e-03,  5.38228545e-03,\n",
       "        -1.72834881e-02,  1.68930292e-02, -1.29944095e-02,  2.37765443e-03,\n",
       "        -2.78379656e-02,  5.54411067e-03, -1.04534840e-02,  2.69825500e-03,\n",
       "        -2.16722041e-02, -1.52968541e-02,  7.71636202e-04, -2.53164656e-02,\n",
       "        -2.75171269e-02,  7.65407039e-03, -4.73390985e-03,  3.97271384e-03,\n",
       "         2.16154102e-02,  3.17888334e-03, -1.02573168e-02, -4.32304386e-03,\n",
       "        -9.00437124e-03, -1.73191559e-02, -6.14399044e-03, -8.29470053e-04,\n",
       "        -2.54225597e-06,  5.80505684e-07,  3.86004757e-07, -2.45404044e-06,\n",
       "        -2.53035228e-06, -1.75495620e-06,  1.20167601e-06, -5.09353470e-07,\n",
       "         2.02472961e-06,  1.05787649e-06,  1.66251277e-06,  8.54564803e-07,\n",
       "        -5.90228228e-06,  8.77756520e-07, -5.90570494e-07, -5.56606983e-06,\n",
       "         2.14879788e-06, -2.89530988e-07,  1.10291535e-06,  2.54147722e-06,\n",
       "        -1.26964494e-06,  5.82859548e-06, -4.23180296e-07,  7.96536710e-07,\n",
       "        -6.26754900e-06,  1.23207258e-06, -1.14857676e-06, -4.13300040e-06,\n",
       "        -3.68919063e-06, -4.14077567e-06,  1.39954909e-05, -3.20355434e-06,\n",
       "         1.11356783e-07, -3.43873398e-06,  1.94401696e-06,  3.84268787e-06,\n",
       "         4.85862165e-06, -3.65394226e-06, -7.34878768e-07,  3.07301571e-06,\n",
       "         1.25088070e-06, -9.87454086e-07, -1.47295088e-06, -5.47692798e-06,\n",
       "        -1.53755150e-06, -1.60902141e-06,  2.81225084e-06,  7.82919187e-07,\n",
       "        -1.87050114e-06,  9.21782885e-07,  9.70223482e-07, -4.48527453e-06,\n",
       "        -1.74643822e-06, -3.39300937e-06,  5.90204763e-07, -1.31701540e-07,\n",
       "         3.86285001e-06, -2.59707622e-06, -8.98204007e-07, -2.06400364e-06,\n",
       "        -1.09704871e-07, -5.93242294e-06,  1.02928652e-06,  2.69495740e-06,\n",
       "        -1.05956701e-06, -3.42527437e-06,  3.37145161e-06,  3.07395112e-06,\n",
       "        -1.51479469e-06, -2.36152141e-06, -7.26774488e-07,  2.40526674e-06,\n",
       "        -3.88988201e-06,  2.16408739e-06,  2.33148421e-06,  3.50265282e-06,\n",
       "        -6.32074079e-06, -2.45304733e-07, -9.78145209e-08,  7.85041811e-06,\n",
       "        -3.42370782e-07,  1.14079555e-07,  3.92548827e-06,  3.19162564e-06,\n",
       "         6.38242091e-06,  4.10916664e-06, -3.80726647e-07,  2.56032990e-06,\n",
       "        -5.45345665e-06,  5.30739158e-07,  1.60649188e-06, -5.18720640e-08,\n",
       "         1.77217498e-06, -1.29231648e-06,  3.95345523e-06,  1.89816944e-06,\n",
       "         9.89217142e-07, -1.11399561e-06, -1.62369668e-06, -2.25230542e-06,\n",
       "        -1.49578318e-06, -1.54588645e-06, -4.04352886e-06,  3.19180731e-06,\n",
       "         8.90452725e-07,  1.43705279e-06,  4.66592564e-06, -3.96204996e-06,\n",
       "         5.43769943e-07,  2.12788927e-06, -2.48232777e-06, -5.65398523e-07,\n",
       "        -7.85028647e-07,  4.09328641e-06,  1.70140538e-06, -1.84776366e-06,\n",
       "        -6.57332794e-06, -1.47486901e-07,  6.26551412e-07,  6.55207828e-07,\n",
       "        -1.33499057e-07,  8.55438543e-07,  3.11926760e-06,  5.36725656e-06,\n",
       "         3.13395117e-06,  6.50240963e-06, -1.76521462e-06, -1.86467446e-06,\n",
       "        -3.14674298e-06,  4.36592188e-07,  1.95218490e-06, -4.48595620e-06,\n",
       "        -1.92223911e-06, -2.77987539e-07, -1.01810133e-06, -3.10796236e-06,\n",
       "         3.11404892e-06,  1.74267825e-06,  1.97044824e-06,  5.02658850e-06,\n",
       "        -3.35499840e-06,  1.67631094e-06, -5.79560719e-08, -4.52592622e-06,\n",
       "         2.23541406e-06, -1.44732411e-07,  1.59671868e-06,  4.81692382e-07,\n",
       "        -5.23324434e-06,  5.22507344e-06,  1.59517060e-06,  6.76964902e-08,\n",
       "        -5.40354358e-06, -1.70391388e-07, -1.17479567e-06, -2.83846180e-06,\n",
       "         9.81199264e-08, -3.57688214e-06,  1.46257989e-05, -1.71495685e-06,\n",
       "         1.22259837e-06, -3.52861571e-06, -1.30540514e-06,  4.87854231e-06,\n",
       "         1.59085801e-06, -4.74821672e-06,  9.94536663e-07,  5.12499582e-06,\n",
       "         1.17864829e-06, -2.66896814e-06, -3.32407353e-06, -6.60900741e-06,\n",
       "        -2.59854596e-06,  2.69081715e-06,  4.80551489e-06,  1.80414304e-06,\n",
       "        -5.76110460e-06, -6.30576267e-08,  1.12273688e-06, -6.64204117e-06,\n",
       "         1.57835871e-07, -4.13234238e-06, -1.39638212e-06, -1.25932343e-06,\n",
       "         5.21233733e-06, -4.43241515e-06, -1.19043614e-06, -1.80794427e-06,\n",
       "        -1.18174125e-06, -4.00170939e-06,  2.72865736e-06,  3.27577368e-06,\n",
       "        -3.51406993e-06, -2.60510637e-06, -3.20545240e-07, -1.59046465e-06,\n",
       "        -3.18872549e-06, -2.48605443e-06, -2.50685139e-06,  3.80768347e-06,\n",
       "        -6.08453547e-06, -2.97842689e-07,  4.70453324e-06,  3.13383453e-06,\n",
       "        -6.72898977e-06,  3.25092606e-06,  1.37044697e-06,  6.83571034e-06,\n",
       "         3.16309092e-06, -9.57602651e-07,  7.44250656e-06,  9.20180838e-08,\n",
       "         6.21220124e-06,  3.77717606e-06,  2.22780130e-07,  3.54814642e-06,\n",
       "        -3.28334704e-06,  2.32213074e-06,  3.03448985e-07,  5.17736680e-08,\n",
       "         2.06103095e-07, -3.67308917e-06,  3.91470849e-06,  2.89400509e-06,\n",
       "         1.96766541e-06, -1.67811038e-06, -1.78646928e-07, -4.87915713e-06,\n",
       "        -3.02194007e-06, -8.54756195e-07, -3.72578666e-06, -1.64131620e-06,\n",
       "         1.63493803e-06,  2.20730112e-06,  6.19028197e-06, -2.81512962e-06,\n",
       "         1.49749297e-08,  2.17404863e-06, -1.64015569e-06, -1.18726621e-06,\n",
       "         6.02461114e-07,  3.80228289e-06, -7.93570507e-08, -2.29102830e-06,\n",
       "        -5.84432200e-06, -2.96605390e-06,  1.87180456e-06,  9.81495305e-07,\n",
       "         2.21754408e-06,  4.59893563e-08,  4.98834015e-06,  5.20407866e-06,\n",
       "         5.98252382e-06,  6.08590426e-06, -1.05588015e-06, -2.50560402e-06],\n",
       "       dtype=float32),\n",
       " array([[ 1.0592348e-04,  9.3284514e-05,  1.7891507e-06, ...,\n",
       "         -8.4917609e-08, -8.5842643e-08, -8.6603436e-08],\n",
       "        [-2.0062194e-04, -3.2235376e-04, -2.3313481e-04, ...,\n",
       "          6.5761924e-07,  6.6486751e-07,  6.7041481e-07],\n",
       "        [ 2.7306021e-05,  1.1090258e-04,  2.3828466e-04, ...,\n",
       "         -2.5738828e-07, -2.6022184e-07, -2.6238149e-07],\n",
       "        ...,\n",
       "        [-9.0111542e-05,  6.6771594e-05,  7.0125916e-06, ...,\n",
       "          1.1368110e-07,  1.1492805e-07,  1.1592704e-07],\n",
       "        [-4.5629600e-05, -8.0288009e-07, -5.2476713e-05, ...,\n",
       "          1.6141673e-08,  1.6273678e-08,  1.6407657e-08],\n",
       "        [-2.1641613e-04, -1.7622863e-04, -5.1887051e-05, ...,\n",
       "          2.9209676e-07,  2.9530301e-07,  2.9780389e-07]], dtype=float32),\n",
       " array([-0.7813241 , -1.0980121 , -0.98131526, ...,  0.0019946 ,\n",
       "         0.00201646,  0.00203345], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>4. Apply the optimizer to the variables / gradients tuple.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ltsm\"></a>\n",
    "<h2>LSTM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:\n",
    "<ul>\n",
    "    <li>We have to create the model in accordance with our defined hyperparameters</li>\n",
    "    <li>We have to create the placeholders for our input data and expected outputs (the real data)</li>\n",
    "    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n",
    "    <li>We have to create the word embeddings and point them to the input data</li>\n",
    "    <li>We have to create the input structure for our RNN</li>\n",
    "    <li>We have to instantiate our RNN model and retrieve the variable in which we should expect our outputs to appear</li>\n",
    "    <li>We need to create a logistic structure to return the probability of our words</li>\n",
    "    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n",
    "    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, action_type):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "            \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n",
    "\n",
    "What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 4297.790 speed: 4235 wps\n",
      "Itr 87 of 774, perplexity: 1274.174 speed: 3752 wps\n",
      "Itr 164 of 774, perplexity: 981.954 speed: 3716 wps\n",
      "Itr 241 of 774, perplexity: 822.352 speed: 3620 wps\n",
      "Itr 318 of 774, perplexity: 726.310 speed: 3475 wps\n",
      "Itr 395 of 774, perplexity: 649.046 speed: 3349 wps\n",
      "Itr 472 of 774, perplexity: 589.173 speed: 3341 wps\n",
      "Itr 549 of 774, perplexity: 535.110 speed: 3364 wps\n",
      "Itr 626 of 774, perplexity: 491.673 speed: 3399 wps\n",
      "Itr 703 of 774, perplexity: 457.439 speed: 3425 wps\n",
      "Epoch 1 : Train Perplexity: 432.822\n",
      "Epoch 1 : Valid Perplexity: 268.135\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 278.635 speed: 3680 wps\n",
      "Itr 87 of 774, perplexity: 239.196 speed: 3585 wps\n",
      "Itr 164 of 774, perplexity: 229.356 speed: 3648 wps\n",
      "Itr 241 of 774, perplexity: 219.967 speed: 3674 wps\n",
      "Itr 318 of 774, perplexity: 217.163 speed: 3690 wps\n",
      "Itr 395 of 774, perplexity: 211.437 speed: 3705 wps\n",
      "Itr 472 of 774, perplexity: 207.157 speed: 3707 wps\n",
      "Itr 549 of 774, perplexity: 200.640 speed: 3713 wps\n",
      "Itr 626 of 774, perplexity: 195.076 speed: 3611 wps\n",
      "Itr 703 of 774, perplexity: 190.843 speed: 3484 wps\n",
      "Epoch 2 : Train Perplexity: 188.060\n",
      "Epoch 2 : Valid Perplexity: 173.626\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 188.845 speed: 2750 wps\n",
      "Itr 87 of 774, perplexity: 161.339 speed: 2919 wps\n",
      "Itr 164 of 774, perplexity: 157.413 speed: 2910 wps\n",
      "Itr 241 of 774, perplexity: 152.767 speed: 2920 wps\n",
      "Itr 318 of 774, perplexity: 152.761 speed: 2923 wps\n",
      "Itr 395 of 774, perplexity: 150.103 speed: 2926 wps\n",
      "Itr 472 of 774, perplexity: 148.503 speed: 2924 wps\n",
      "Itr 549 of 774, perplexity: 144.842 speed: 2924 wps\n",
      "Itr 626 of 774, perplexity: 141.964 speed: 2921 wps\n",
      "Itr 703 of 774, perplexity: 140.118 speed: 2920 wps\n",
      "Epoch 3 : Train Perplexity: 139.029\n",
      "Epoch 3 : Valid Perplexity: 151.665\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 150.715 speed: 2840 wps\n",
      "Itr 87 of 774, perplexity: 128.254 speed: 2927 wps\n",
      "Itr 164 of 774, perplexity: 126.344 speed: 2866 wps\n",
      "Itr 241 of 774, perplexity: 123.066 speed: 2781 wps\n",
      "Itr 318 of 774, perplexity: 123.689 speed: 2781 wps\n",
      "Itr 395 of 774, perplexity: 121.855 speed: 2803 wps\n",
      "Itr 472 of 774, perplexity: 120.982 speed: 2809 wps\n",
      "Itr 549 of 774, perplexity: 118.159 speed: 2824 wps\n",
      "Itr 626 of 774, perplexity: 116.181 speed: 2808 wps\n",
      "Itr 703 of 774, perplexity: 115.120 speed: 2791 wps\n",
      "Epoch 4 : Train Perplexity: 114.581\n",
      "Epoch 4 : Valid Perplexity: 142.345\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 128.130 speed: 2037 wps\n"
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower Perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
