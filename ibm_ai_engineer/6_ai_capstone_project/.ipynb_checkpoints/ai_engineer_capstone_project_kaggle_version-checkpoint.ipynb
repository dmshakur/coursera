{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from timeit import Timer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.nn.init import kaiming_normal_\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is a GPU being utilized and what model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.get_device_name(0), torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset, in raw image form.\n",
    "40,000 images total.\n",
    "Half labeled positive, for concrete with cracks and the other half labeled negative, for concrete without cracks.\n",
    "\n",
    "Remove all data first, then download and unzip the data in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-24 18:00:17--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 245259777 (234M) [application/zip]\n",
      "Saving to: ‘data/concrete_data.zip’\n",
      "\n",
      "concrete_data.zip   100%[===================>] 233.90M  30.7MB/s    in 7.8s    \n",
      "\n",
      "2020-05-24 18:00:25 (29.9 MB/s) - ‘data/concrete_data.zip’ saved [245259777/245259777]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf *\n",
    "# !mkdir data\n",
    "# !wget -P data -O data/concrete_data.zip https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
    "# !unzip -q -d data data/concrete_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "As it turns out I didn't need to create a dataset object and fiddle with the data myself, at least not very much. So I commented out the `class Data`, but just left it here anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Data(Dataset):\n",
    "#     def __init__(self, transforms, train = False):\n",
    "#         self.x = torch.zeros(1)\n",
    "#         self.y = torch.zeros(1)\n",
    "        \n",
    "#         negative_file_path = os.path.join('concrete_class/', 'Negative')\n",
    "#         negative_files = [file for file in os.listdir(negative_file_path) if file.endswith('.jpg')]\n",
    "#         negative_files.sort()\n",
    "#         positive_file_path = os.path.join('concrete_class/', 'Positive')\n",
    "#         positive_files = [file for file in os.listdir(positive_file_path) if file.endswith('.jpg')]\n",
    "#         positive_files.sort()\n",
    "#         print(negative_files[0], type(negative_files[0]))\n",
    "        \n",
    "#         if train:\n",
    "#             self.x = torch.zeros(36000, 1)\n",
    "#             for i in range(36000):\n",
    "#                 self.x[i] = transforms(negative_files[i]) if i < 18000 else transforms(negative_files[i])\n",
    "#             self.y = torch.zeros(36000, 1, dtype = 'uint8')\n",
    "#             self.y[18000:, :] = 1\n",
    "#         else:\n",
    "#             self.x = torch.zeros(4000, 1)\n",
    "#             for i in range(4000):\n",
    "#                 self.x[i] = transforms(negative_files[i + 36000]) if i < 2000 else transforms(negative_files[i + 38000])\n",
    "#             self.y = torch.zeros(4000, 1, dtype = 'uint8')\n",
    "#             self.y[2000:, :] = 1\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.y[index], self.x[index]\n",
    "#     def __len__(self):\n",
    "#         return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network.\n",
    "This network has 3 convolutional layers, 2 fully connected layers and a fully connected output layer. Each convolutional layer has batch normalization and max-pooling and each fully connected layer has dropout except for the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_1 = 16, out_2 = 32, out_3 = 64):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5)\n",
    "        # self.drop2d1 = nn.Dropout2d(p = 0.35, inplace = True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_1, momentum = 0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5)\n",
    "        # self.drop2d2 = nn.Dropout2d(p = 0.35, inplace = True)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_2, momentum = 0.5)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = out_2, out_channels = out_3, kernel_size = 5)\n",
    "        # self.drop2d3 = nn.Dropout2d(p = 0.35, inplace = True)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size = 2, stride = 1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(out_3, momentum = 0.5)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear1 = nn.Linear(2876416, 200)\n",
    "        self.drop1 = nn.Dropout(p = 0.35, inplace = True)\n",
    "        self.linear2 = nn.Linear(200, 100)\n",
    "        self.drop2 = nn.Dropout(p = 0.35, inplace = True)\n",
    "        self.linear_output = nn.Linear(100, 10)\n",
    "\n",
    "        kaiming_normal_(self.conv1.weight, nonlinearity = 'relu')\n",
    "        kaiming_normal_(self.conv2.weight, nonlinearity = 'relu')\n",
    "        kaiming_normal_(self.linear1.weight, nonlinearity = 'relu')\n",
    "        kaiming_normal_(self.linear2.weight, nonlinearity = 'relu')\n",
    "        kaiming_normal_(self.linear_output.weight, nonlinearity = 'relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.drop2d1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        # x = self.drop2d2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        # x = self.drop2d3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.linear_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs):\n",
    "    loss_acc = {'training_loss': [], 'training_acc': [], 'validation_loss': [], 'validation_acc': []}\n",
    "    for epoch in range(epochs):\n",
    "        tr_time = Timer()\n",
    "        tr_acc = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model.forward(x)\n",
    "            tr_loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_acc.training_loss.append(tr_loss)\n",
    "            \n",
    "        tr_acc = 100 * (tr_acc / len(train_loader))\n",
    "        loss_acc.training_acc.append(tr_acc)\n",
    "        \n",
    "        val_time = Timer()\n",
    "        for x, y in validation_loader:\n",
    "            model.eval()\n",
    "            z = model(x)\n",
    "            val_loss = criterion(z, y)\n",
    "            _, y_hat = torch.max(z, 1)\n",
    "            val_acc += (y_hat == y).sum().item()\n",
    "            loss_acc.validation_loss.append(val_loss)\n",
    "            \n",
    "        val_acc = 100 * (val_acc / len(train_loader))\n",
    "        loss_acc.validation_acc.append(val_acc)\n",
    "        print('Epoch: {}, training accuracy/time: {}, {}, validation accuracy/time: {}, {}'\n",
    "              .format(epoch, tr_acc, tr_time.timeit(), val_acc, val_timer.timeit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToCudaTensor(transforms.ToTensor):\n",
    "    def __call__(self, image):\n",
    "        img = np.asarray(image)\n",
    "        img = img.reshape((3, 227, 227))\n",
    "        return torch.from_numpy(img).float().to(0)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = transforms.ToTensor()\n",
    "# train_data = Data(transforms = transformer, train = True)\n",
    "# val_data = Data(transforms = transformer, train = False)\n",
    "# images = datasets.ImageFolder(root = '/', transform = ToCudaTensor())\n",
    "images = datasets.ImageFolder(root = '/', transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(images, shuffle = True)\n",
    "val_loader = DataLoader(images, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(images[0][0]), images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "# model.cuda(0)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_acc_data = train(model, criterion, optimizer, train_loader, val_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
